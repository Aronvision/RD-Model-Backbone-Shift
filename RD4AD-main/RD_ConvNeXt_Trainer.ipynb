{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure ConvNeXt RD Model Trainer\n",
    "완전히 ConvNeXt 아키텍처로 설계된 RD 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dataset import get_data_transforms\n",
    "from RD_ConvNeXt_Model import rd_convnext_model, convnext_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_convnext_rd(class_name='bottle', epochs=100, batch_size=8, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Pure ConvNeXt RD 모델 학습\n",
    "    \"\"\"\n",
    "    print(f\"Training Pure ConvNeXt RD Model on {class_name}\")\n",
    "    print(f\"Epochs: {epochs}, Batch Size: {batch_size}, LR: {learning_rate}\")\n",
    "    \n",
    "    # Device setup\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Data setup\n",
    "    data_transform = get_data_transforms(256, 256)\n",
    "    train_path = f'./data/{class_name}/train'\n",
    "    train_data = ImageFolder(root=train_path, transform=data_transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    \n",
    "    # Model setup\n",
    "    model = rd_convnext_model(pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer - only train BN layer + Decoder\n",
    "    trainable_params = model.get_trainable_params()\n",
    "    optimizer = optim.AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    model.freeze_encoder()  # Ensure encoder is frozen\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            teacher_features, student_features = model(images)\n",
    "            \n",
    "            # Loss calculation with normalization\n",
    "            loss = convnext_loss_function(teacher_features, student_features, normalize=True)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_time = time.time() - start_time\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Complete:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.6f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': {\n",
    "                    'bn_layer': model.bn_layer.state_dict(),\n",
    "                    'decoder': model.decoder.state_dict()\n",
    "                },\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, f'./checkpoints/convnext_pure_{class_name}.pth')\n",
    "            print(f\"  → Best model saved! (Loss: {best_loss:.6f})\")\n",
    "        \n",
    "        # Save checkpoint every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': {\n",
    "                    'bn_layer': model.bn_layer.state_dict(),\n",
    "                    'decoder': model.decoder.state_dict()\n",
    "                },\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, f'./checkpoints/convnext_pure_{class_name}_epoch_{epoch+1}.pth')\n",
    "            print(f\"  → Checkpoint saved at epoch {epoch+1}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training completed for {class_name}!\")\n",
    "    print(f\"Best loss: {best_loss:.6f}\")\n",
    "    print(f\"Model saved as: convnext_pure_{class_name}.pth\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pure ConvNeXt RD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pure ConvNeXt RD Model on bottle\n",
      "Epochs: 100, Batch Size: 8, LR: 0.001\n",
      "Device: cuda\n",
      "Training samples: 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aron\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aron\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 8,727,456\n",
      "Epoch [1/100], Batch [0/27], Loss: 0.979213\n",
      "Epoch [1/100], Batch [10/27], Loss: 0.143903\n",
      "Epoch [1/100], Batch [20/27], Loss: 0.094693\n",
      "Epoch [1/100] Complete:\n",
      "  Average Loss: 0.202887\n",
      "  Time: 10.09s\n",
      "  Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.202887)\n",
      "Epoch [2/100], Batch [0/27], Loss: 0.083077\n",
      "Epoch [2/100], Batch [10/27], Loss: 0.073167\n",
      "Epoch [2/100], Batch [20/27], Loss: 0.069505\n",
      "Epoch [2/100] Complete:\n",
      "  Average Loss: 0.072273\n",
      "  Time: 6.62s\n",
      "  Learning Rate: 0.000999\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.072273)\n",
      "Epoch [3/100], Batch [0/27], Loss: 0.064024\n",
      "Epoch [3/100], Batch [10/27], Loss: 0.061800\n",
      "Epoch [3/100], Batch [20/27], Loss: 0.062427\n",
      "Epoch [3/100] Complete:\n",
      "  Average Loss: 0.062909\n",
      "  Time: 6.66s\n",
      "  Learning Rate: 0.000998\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.062909)\n",
      "Epoch [4/100], Batch [0/27], Loss: 0.060568\n",
      "Epoch [4/100], Batch [10/27], Loss: 0.057561\n",
      "Epoch [4/100], Batch [20/27], Loss: 0.056114\n",
      "Epoch [4/100] Complete:\n",
      "  Average Loss: 0.058227\n",
      "  Time: 6.66s\n",
      "  Learning Rate: 0.000996\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.058227)\n",
      "Epoch [5/100], Batch [0/27], Loss: 0.056000\n",
      "Epoch [5/100], Batch [10/27], Loss: 0.054916\n",
      "Epoch [5/100], Batch [20/27], Loss: 0.055414\n",
      "Epoch [5/100] Complete:\n",
      "  Average Loss: 0.055926\n",
      "  Time: 6.64s\n",
      "  Learning Rate: 0.000994\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.055926)\n",
      "Epoch [6/100], Batch [0/27], Loss: 0.054021\n",
      "Epoch [6/100], Batch [10/27], Loss: 0.056800\n",
      "Epoch [6/100], Batch [20/27], Loss: 0.055740\n",
      "Epoch [6/100] Complete:\n",
      "  Average Loss: 0.054603\n",
      "  Time: 6.64s\n",
      "  Learning Rate: 0.000991\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.054603)\n",
      "Epoch [7/100], Batch [0/27], Loss: 0.054405\n",
      "Epoch [7/100], Batch [10/27], Loss: 0.050640\n",
      "Epoch [7/100], Batch [20/27], Loss: 0.048986\n",
      "Epoch [7/100] Complete:\n",
      "  Average Loss: 0.050878\n",
      "  Time: 6.65s\n",
      "  Learning Rate: 0.000988\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.050878)\n",
      "Epoch [8/100], Batch [0/27], Loss: 0.048592\n",
      "Epoch [8/100], Batch [10/27], Loss: 0.046455\n",
      "Epoch [8/100], Batch [20/27], Loss: 0.045729\n",
      "Epoch [8/100] Complete:\n",
      "  Average Loss: 0.051877\n",
      "  Time: 6.50s\n",
      "  Learning Rate: 0.000984\n",
      "--------------------------------------------------\n",
      "Epoch [9/100], Batch [0/27], Loss: 0.053781\n",
      "Epoch [9/100], Batch [10/27], Loss: 0.049133\n",
      "Epoch [9/100], Batch [20/27], Loss: 0.052199\n",
      "Epoch [9/100] Complete:\n",
      "  Average Loss: 0.048592\n",
      "  Time: 6.63s\n",
      "  Learning Rate: 0.000980\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.048592)\n",
      "Epoch [10/100], Batch [0/27], Loss: 0.040714\n",
      "Epoch [10/100], Batch [10/27], Loss: 0.043514\n",
      "Epoch [10/100], Batch [20/27], Loss: 0.037837\n",
      "Epoch [10/100] Complete:\n",
      "  Average Loss: 0.040481\n",
      "  Time: 6.62s\n",
      "  Learning Rate: 0.000976\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.040481)\n",
      "Epoch [11/100], Batch [0/27], Loss: 0.037143\n",
      "Epoch [11/100], Batch [10/27], Loss: 0.039349\n",
      "Epoch [11/100], Batch [20/27], Loss: 0.035180\n",
      "Epoch [11/100] Complete:\n",
      "  Average Loss: 0.037379\n",
      "  Time: 6.65s\n",
      "  Learning Rate: 0.000970\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.037379)\n",
      "Epoch [12/100], Batch [0/27], Loss: 0.034664\n",
      "Epoch [12/100], Batch [10/27], Loss: 0.032560\n",
      "Epoch [12/100], Batch [20/27], Loss: 0.034347\n",
      "Epoch [12/100] Complete:\n",
      "  Average Loss: 0.034543\n",
      "  Time: 6.27s\n",
      "  Learning Rate: 0.000965\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.034543)\n",
      "Epoch [13/100], Batch [0/27], Loss: 0.034528\n",
      "Epoch [13/100], Batch [10/27], Loss: 0.037907\n",
      "Epoch [13/100], Batch [20/27], Loss: 0.032997\n",
      "Epoch [13/100] Complete:\n",
      "  Average Loss: 0.033043\n",
      "  Time: 6.93s\n",
      "  Learning Rate: 0.000959\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.033043)\n",
      "Epoch [14/100], Batch [0/27], Loss: 0.034715\n",
      "Epoch [14/100], Batch [10/27], Loss: 0.029573\n",
      "Epoch [14/100], Batch [20/27], Loss: 0.042980\n",
      "Epoch [14/100] Complete:\n",
      "  Average Loss: 0.031118\n",
      "  Time: 6.72s\n",
      "  Learning Rate: 0.000952\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.031118)\n",
      "Epoch [15/100], Batch [0/27], Loss: 0.028968\n",
      "Epoch [15/100], Batch [10/27], Loss: 0.027869\n",
      "Epoch [15/100], Batch [20/27], Loss: 0.029329\n",
      "Epoch [15/100] Complete:\n",
      "  Average Loss: 0.029906\n",
      "  Time: 6.46s\n",
      "  Learning Rate: 0.000946\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.029906)\n",
      "Epoch [16/100], Batch [0/27], Loss: 0.027391\n",
      "Epoch [16/100], Batch [10/27], Loss: 0.026847\n",
      "Epoch [16/100], Batch [20/27], Loss: 0.036013\n",
      "Epoch [16/100] Complete:\n",
      "  Average Loss: 0.028206\n",
      "  Time: 6.56s\n",
      "  Learning Rate: 0.000938\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.028206)\n",
      "Epoch [17/100], Batch [0/27], Loss: 0.048132\n",
      "Epoch [17/100], Batch [10/27], Loss: 0.030657\n",
      "Epoch [17/100], Batch [20/27], Loss: 0.028182\n",
      "Epoch [17/100] Complete:\n",
      "  Average Loss: 0.033463\n",
      "  Time: 6.32s\n",
      "  Learning Rate: 0.000930\n",
      "--------------------------------------------------\n",
      "Epoch [18/100], Batch [0/27], Loss: 0.027893\n",
      "Epoch [18/100], Batch [10/27], Loss: 0.026192\n",
      "Epoch [18/100], Batch [20/27], Loss: 0.027895\n",
      "Epoch [18/100] Complete:\n",
      "  Average Loss: 0.027826\n",
      "  Time: 6.63s\n",
      "  Learning Rate: 0.000922\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.027826)\n",
      "Epoch [19/100], Batch [0/27], Loss: 0.027258\n",
      "Epoch [19/100], Batch [10/27], Loss: 0.026386\n",
      "Epoch [19/100], Batch [20/27], Loss: 0.024817\n",
      "Epoch [19/100] Complete:\n",
      "  Average Loss: 0.026887\n",
      "  Time: 6.42s\n",
      "  Learning Rate: 0.000914\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.026887)\n",
      "Epoch [20/100], Batch [0/27], Loss: 0.024610\n",
      "Epoch [20/100], Batch [10/27], Loss: 0.026204\n",
      "Epoch [20/100], Batch [20/27], Loss: 0.032788\n",
      "Epoch [20/100] Complete:\n",
      "  Average Loss: 0.026282\n",
      "  Time: 6.38s\n",
      "  Learning Rate: 0.000905\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.026282)\n",
      "  → Checkpoint saved at epoch 20\n",
      "Epoch [21/100], Batch [0/27], Loss: 0.025105\n",
      "Epoch [21/100], Batch [10/27], Loss: 0.023621\n",
      "Epoch [21/100], Batch [20/27], Loss: 0.024444\n",
      "Epoch [21/100] Complete:\n",
      "  Average Loss: 0.025339\n",
      "  Time: 6.67s\n",
      "  Learning Rate: 0.000895\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.025339)\n",
      "Epoch [22/100], Batch [0/27], Loss: 0.024578\n",
      "Epoch [22/100], Batch [10/27], Loss: 0.023033\n",
      "Epoch [22/100], Batch [20/27], Loss: 0.028175\n",
      "Epoch [22/100] Complete:\n",
      "  Average Loss: 0.024209\n",
      "  Time: 6.70s\n",
      "  Learning Rate: 0.000885\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.024209)\n",
      "Epoch [23/100], Batch [0/27], Loss: 0.022235\n",
      "Epoch [23/100], Batch [10/27], Loss: 0.022816\n",
      "Epoch [23/100], Batch [20/27], Loss: 0.024418\n",
      "Epoch [23/100] Complete:\n",
      "  Average Loss: 0.024405\n",
      "  Time: 6.64s\n",
      "  Learning Rate: 0.000875\n",
      "--------------------------------------------------\n",
      "Epoch [24/100], Batch [0/27], Loss: 0.024451\n",
      "Epoch [24/100], Batch [10/27], Loss: 0.022172\n",
      "Epoch [24/100], Batch [20/27], Loss: 0.022607\n",
      "Epoch [24/100] Complete:\n",
      "  Average Loss: 0.023705\n",
      "  Time: 6.43s\n",
      "  Learning Rate: 0.000864\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.023705)\n",
      "Epoch [25/100], Batch [0/27], Loss: 0.027706\n",
      "Epoch [25/100], Batch [10/27], Loss: 0.022677\n",
      "Epoch [25/100], Batch [20/27], Loss: 0.021509\n",
      "Epoch [25/100] Complete:\n",
      "  Average Loss: 0.022424\n",
      "  Time: 6.25s\n",
      "  Learning Rate: 0.000854\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.022424)\n",
      "Epoch [26/100], Batch [0/27], Loss: 0.020894\n",
      "Epoch [26/100], Batch [10/27], Loss: 0.022532\n",
      "Epoch [26/100], Batch [20/27], Loss: 0.020972\n",
      "Epoch [26/100] Complete:\n",
      "  Average Loss: 0.022477\n",
      "  Time: 6.51s\n",
      "  Learning Rate: 0.000842\n",
      "--------------------------------------------------\n",
      "Epoch [27/100], Batch [0/27], Loss: 0.021268\n",
      "Epoch [27/100], Batch [10/27], Loss: 0.020413\n",
      "Epoch [27/100], Batch [20/27], Loss: 0.021678\n",
      "Epoch [27/100] Complete:\n",
      "  Average Loss: 0.022484\n",
      "  Time: 6.86s\n",
      "  Learning Rate: 0.000831\n",
      "--------------------------------------------------\n",
      "Epoch [28/100], Batch [0/27], Loss: 0.027634\n",
      "Epoch [28/100], Batch [10/27], Loss: 0.021039\n",
      "Epoch [28/100], Batch [20/27], Loss: 0.021760\n",
      "Epoch [28/100] Complete:\n",
      "  Average Loss: 0.022245\n",
      "  Time: 6.91s\n",
      "  Learning Rate: 0.000819\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.022245)\n",
      "Epoch [29/100], Batch [0/27], Loss: 0.022977\n",
      "Epoch [29/100], Batch [10/27], Loss: 0.026292\n",
      "Epoch [29/100], Batch [20/27], Loss: 0.021583\n",
      "Epoch [29/100] Complete:\n",
      "  Average Loss: 0.022779\n",
      "  Time: 7.86s\n",
      "  Learning Rate: 0.000806\n",
      "--------------------------------------------------\n",
      "Epoch [30/100], Batch [0/27], Loss: 0.020860\n",
      "Epoch [30/100], Batch [10/27], Loss: 0.020251\n",
      "Epoch [30/100], Batch [20/27], Loss: 0.029507\n",
      "Epoch [30/100] Complete:\n",
      "  Average Loss: 0.022140\n",
      "  Time: 8.01s\n",
      "  Learning Rate: 0.000794\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.022140)\n",
      "Epoch [31/100], Batch [0/27], Loss: 0.020326\n",
      "Epoch [31/100], Batch [10/27], Loss: 0.020440\n",
      "Epoch [31/100], Batch [20/27], Loss: 0.020172\n",
      "Epoch [31/100] Complete:\n",
      "  Average Loss: 0.020417\n",
      "  Time: 8.43s\n",
      "  Learning Rate: 0.000781\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.020417)\n",
      "Epoch [32/100], Batch [0/27], Loss: 0.025518\n",
      "Epoch [32/100], Batch [10/27], Loss: 0.020404\n",
      "Epoch [32/100], Batch [20/27], Loss: 0.019827\n",
      "Epoch [32/100] Complete:\n",
      "  Average Loss: 0.020706\n",
      "  Time: 7.14s\n",
      "  Learning Rate: 0.000768\n",
      "--------------------------------------------------\n",
      "Epoch [33/100], Batch [0/27], Loss: 0.019800\n",
      "Epoch [33/100], Batch [10/27], Loss: 0.020184\n",
      "Epoch [33/100], Batch [20/27], Loss: 0.018972\n",
      "Epoch [33/100] Complete:\n",
      "  Average Loss: 0.020219\n",
      "  Time: 6.63s\n",
      "  Learning Rate: 0.000755\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.020219)\n",
      "Epoch [34/100], Batch [0/27], Loss: 0.019587\n",
      "Epoch [34/100], Batch [10/27], Loss: 0.025531\n",
      "Epoch [34/100], Batch [20/27], Loss: 0.020411\n",
      "Epoch [34/100] Complete:\n",
      "  Average Loss: 0.020194\n",
      "  Time: 6.88s\n",
      "  Learning Rate: 0.000741\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.020194)\n",
      "Epoch [35/100], Batch [0/27], Loss: 0.019699\n",
      "Epoch [35/100], Batch [10/27], Loss: 0.027483\n",
      "Epoch [35/100], Batch [20/27], Loss: 0.020343\n",
      "Epoch [35/100] Complete:\n",
      "  Average Loss: 0.020579\n",
      "  Time: 10.92s\n",
      "  Learning Rate: 0.000727\n",
      "--------------------------------------------------\n",
      "Epoch [36/100], Batch [0/27], Loss: 0.020988\n",
      "Epoch [36/100], Batch [10/27], Loss: 0.020085\n",
      "Epoch [36/100], Batch [20/27], Loss: 0.018729\n",
      "Epoch [36/100] Complete:\n",
      "  Average Loss: 0.020269\n",
      "  Time: 16.06s\n",
      "  Learning Rate: 0.000713\n",
      "--------------------------------------------------\n",
      "Epoch [37/100], Batch [0/27], Loss: 0.018863\n",
      "Epoch [37/100], Batch [10/27], Loss: 0.018575\n",
      "Epoch [37/100], Batch [20/27], Loss: 0.019332\n",
      "Epoch [37/100] Complete:\n",
      "  Average Loss: 0.020108\n",
      "  Time: 16.83s\n",
      "  Learning Rate: 0.000699\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.020108)\n",
      "Epoch [38/100], Batch [0/27], Loss: 0.024831\n",
      "Epoch [38/100], Batch [10/27], Loss: 0.018524\n",
      "Epoch [38/100], Batch [20/27], Loss: 0.018598\n",
      "Epoch [38/100] Complete:\n",
      "  Average Loss: 0.019152\n",
      "  Time: 16.47s\n",
      "  Learning Rate: 0.000684\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.019152)\n",
      "Epoch [39/100], Batch [0/27], Loss: 0.017601\n",
      "Epoch [39/100], Batch [10/27], Loss: 0.018921\n",
      "Epoch [39/100], Batch [20/27], Loss: 0.018188\n",
      "Epoch [39/100] Complete:\n",
      "  Average Loss: 0.018834\n",
      "  Time: 15.51s\n",
      "  Learning Rate: 0.000669\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.018834)\n",
      "Epoch [40/100], Batch [0/27], Loss: 0.019590\n",
      "Epoch [40/100], Batch [10/27], Loss: 0.018065\n",
      "Epoch [40/100], Batch [20/27], Loss: 0.019739\n",
      "Epoch [40/100] Complete:\n",
      "  Average Loss: 0.019982\n",
      "  Time: 17.45s\n",
      "  Learning Rate: 0.000655\n",
      "--------------------------------------------------\n",
      "  → Checkpoint saved at epoch 40\n",
      "Epoch [41/100], Batch [0/27], Loss: 0.019737\n",
      "Epoch [41/100], Batch [10/27], Loss: 0.019170\n",
      "Epoch [41/100], Batch [20/27], Loss: 0.028176\n",
      "Epoch [41/100] Complete:\n",
      "  Average Loss: 0.019387\n",
      "  Time: 15.52s\n",
      "  Learning Rate: 0.000639\n",
      "--------------------------------------------------\n",
      "Epoch [42/100], Batch [0/27], Loss: 0.018403\n",
      "Epoch [42/100], Batch [10/27], Loss: 0.018102\n",
      "Epoch [42/100], Batch [20/27], Loss: 0.022397\n",
      "Epoch [42/100] Complete:\n",
      "  Average Loss: 0.018774\n",
      "  Time: 17.40s\n",
      "  Learning Rate: 0.000624\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.018774)\n",
      "Epoch [43/100], Batch [0/27], Loss: 0.018413\n",
      "Epoch [43/100], Batch [10/27], Loss: 0.018701\n",
      "Epoch [43/100], Batch [20/27], Loss: 0.017390\n",
      "Epoch [43/100] Complete:\n",
      "  Average Loss: 0.018863\n",
      "  Time: 16.99s\n",
      "  Learning Rate: 0.000609\n",
      "--------------------------------------------------\n",
      "Epoch [44/100], Batch [0/27], Loss: 0.017909\n",
      "Epoch [44/100], Batch [10/27], Loss: 0.017522\n",
      "Epoch [44/100], Batch [20/27], Loss: 0.017601\n",
      "Epoch [44/100] Complete:\n",
      "  Average Loss: 0.018846\n",
      "  Time: 15.40s\n",
      "  Learning Rate: 0.000594\n",
      "--------------------------------------------------\n",
      "Epoch [45/100], Batch [0/27], Loss: 0.017716\n",
      "Epoch [45/100], Batch [10/27], Loss: 0.023275\n",
      "Epoch [45/100], Batch [20/27], Loss: 0.022558\n",
      "Epoch [45/100] Complete:\n",
      "  Average Loss: 0.018805\n",
      "  Time: 15.16s\n",
      "  Learning Rate: 0.000578\n",
      "--------------------------------------------------\n",
      "Epoch [46/100], Batch [0/27], Loss: 0.024359\n",
      "Epoch [46/100], Batch [10/27], Loss: 0.017564\n",
      "Epoch [46/100], Batch [20/27], Loss: 0.017935\n",
      "Epoch [46/100] Complete:\n",
      "  Average Loss: 0.019307\n",
      "  Time: 14.65s\n",
      "  Learning Rate: 0.000563\n",
      "--------------------------------------------------\n",
      "Epoch [47/100], Batch [0/27], Loss: 0.018197\n",
      "Epoch [47/100], Batch [10/27], Loss: 0.017358\n",
      "Epoch [47/100], Batch [20/27], Loss: 0.019583\n",
      "Epoch [47/100] Complete:\n",
      "  Average Loss: 0.018627\n",
      "  Time: 15.20s\n",
      "  Learning Rate: 0.000547\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.018627)\n",
      "Epoch [48/100], Batch [0/27], Loss: 0.017348\n",
      "Epoch [48/100], Batch [10/27], Loss: 0.026412\n",
      "Epoch [48/100], Batch [20/27], Loss: 0.018067\n",
      "Epoch [48/100] Complete:\n",
      "  Average Loss: 0.019004\n",
      "  Time: 14.35s\n",
      "  Learning Rate: 0.000531\n",
      "--------------------------------------------------\n",
      "Epoch [49/100], Batch [0/27], Loss: 0.017671\n",
      "Epoch [49/100], Batch [10/27], Loss: 0.023213\n",
      "Epoch [49/100], Batch [20/27], Loss: 0.018424\n",
      "Epoch [49/100] Complete:\n",
      "  Average Loss: 0.018506\n",
      "  Time: 7.11s\n",
      "  Learning Rate: 0.000516\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.018506)\n",
      "Epoch [50/100], Batch [0/27], Loss: 0.017282\n",
      "Epoch [50/100], Batch [10/27], Loss: 0.021682\n",
      "Epoch [50/100], Batch [20/27], Loss: 0.016752\n",
      "Epoch [50/100] Complete:\n",
      "  Average Loss: 0.018791\n",
      "  Time: 6.37s\n",
      "  Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Epoch [51/100], Batch [0/27], Loss: 0.017186\n",
      "Epoch [51/100], Batch [10/27], Loss: 0.017149\n",
      "Epoch [51/100], Batch [20/27], Loss: 0.016399\n",
      "Epoch [51/100] Complete:\n",
      "  Average Loss: 0.017432\n",
      "  Time: 6.66s\n",
      "  Learning Rate: 0.000484\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.017432)\n",
      "Epoch [52/100], Batch [0/27], Loss: 0.018034\n",
      "Epoch [52/100], Batch [10/27], Loss: 0.018152\n",
      "Epoch [52/100], Batch [20/27], Loss: 0.017438\n",
      "Epoch [52/100] Complete:\n",
      "  Average Loss: 0.018478\n",
      "  Time: 6.99s\n",
      "  Learning Rate: 0.000469\n",
      "--------------------------------------------------\n",
      "Epoch [53/100], Batch [0/27], Loss: 0.016804\n",
      "Epoch [53/100], Batch [10/27], Loss: 0.017248\n",
      "Epoch [53/100], Batch [20/27], Loss: 0.016266\n",
      "Epoch [53/100] Complete:\n",
      "  Average Loss: 0.017933\n",
      "  Time: 12.74s\n",
      "  Learning Rate: 0.000453\n",
      "--------------------------------------------------\n",
      "Epoch [54/100], Batch [0/27], Loss: 0.017546\n",
      "Epoch [54/100], Batch [10/27], Loss: 0.016730\n",
      "Epoch [54/100], Batch [20/27], Loss: 0.016746\n",
      "Epoch [54/100] Complete:\n",
      "  Average Loss: 0.017690\n",
      "  Time: 15.50s\n",
      "  Learning Rate: 0.000437\n",
      "--------------------------------------------------\n",
      "Epoch [55/100], Batch [0/27], Loss: 0.016925\n",
      "Epoch [55/100], Batch [10/27], Loss: 0.016271\n",
      "Epoch [55/100], Batch [20/27], Loss: 0.017230\n",
      "Epoch [55/100] Complete:\n",
      "  Average Loss: 0.016961\n",
      "  Time: 15.19s\n",
      "  Learning Rate: 0.000422\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016961)\n",
      "Epoch [56/100], Batch [0/27], Loss: 0.017172\n",
      "Epoch [56/100], Batch [10/27], Loss: 0.016374\n",
      "Epoch [56/100], Batch [20/27], Loss: 0.016689\n",
      "Epoch [56/100] Complete:\n",
      "  Average Loss: 0.016869\n",
      "  Time: 16.02s\n",
      "  Learning Rate: 0.000406\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016869)\n",
      "Epoch [57/100], Batch [0/27], Loss: 0.016852\n",
      "Epoch [57/100], Batch [10/27], Loss: 0.018440\n",
      "Epoch [57/100], Batch [20/27], Loss: 0.017034\n",
      "Epoch [57/100] Complete:\n",
      "  Average Loss: 0.018139\n",
      "  Time: 15.96s\n",
      "  Learning Rate: 0.000391\n",
      "--------------------------------------------------\n",
      "Epoch [58/100], Batch [0/27], Loss: 0.018394\n",
      "Epoch [58/100], Batch [10/27], Loss: 0.017178\n",
      "Epoch [58/100], Batch [20/27], Loss: 0.017157\n",
      "Epoch [58/100] Complete:\n",
      "  Average Loss: 0.017986\n",
      "  Time: 17.51s\n",
      "  Learning Rate: 0.000376\n",
      "--------------------------------------------------\n",
      "Epoch [59/100], Batch [0/27], Loss: 0.016785\n",
      "Epoch [59/100], Batch [10/27], Loss: 0.016550\n",
      "Epoch [59/100], Batch [20/27], Loss: 0.016600\n",
      "Epoch [59/100] Complete:\n",
      "  Average Loss: 0.016987\n",
      "  Time: 16.02s\n",
      "  Learning Rate: 0.000361\n",
      "--------------------------------------------------\n",
      "Epoch [60/100], Batch [0/27], Loss: 0.016388\n",
      "Epoch [60/100], Batch [10/27], Loss: 0.016044\n",
      "Epoch [60/100], Batch [20/27], Loss: 0.016337\n",
      "Epoch [60/100] Complete:\n",
      "  Average Loss: 0.017583\n",
      "  Time: 16.83s\n",
      "  Learning Rate: 0.000345\n",
      "--------------------------------------------------\n",
      "  → Checkpoint saved at epoch 60\n",
      "Epoch [61/100], Batch [0/27], Loss: 0.024348\n",
      "Epoch [61/100], Batch [10/27], Loss: 0.020752\n",
      "Epoch [61/100], Batch [20/27], Loss: 0.017191\n",
      "Epoch [61/100] Complete:\n",
      "  Average Loss: 0.018073\n",
      "  Time: 14.78s\n",
      "  Learning Rate: 0.000331\n",
      "--------------------------------------------------\n",
      "Epoch [62/100], Batch [0/27], Loss: 0.016533\n",
      "Epoch [62/100], Batch [10/27], Loss: 0.016329\n",
      "Epoch [62/100], Batch [20/27], Loss: 0.016637\n",
      "Epoch [62/100] Complete:\n",
      "  Average Loss: 0.017010\n",
      "  Time: 17.02s\n",
      "  Learning Rate: 0.000316\n",
      "--------------------------------------------------\n",
      "Epoch [63/100], Batch [0/27], Loss: 0.015506\n",
      "Epoch [63/100], Batch [10/27], Loss: 0.017062\n",
      "Epoch [63/100], Batch [20/27], Loss: 0.019996\n",
      "Epoch [63/100] Complete:\n",
      "  Average Loss: 0.017043\n",
      "  Time: 17.32s\n",
      "  Learning Rate: 0.000301\n",
      "--------------------------------------------------\n",
      "Epoch [64/100], Batch [0/27], Loss: 0.019898\n",
      "Epoch [64/100], Batch [10/27], Loss: 0.017451\n",
      "Epoch [64/100], Batch [20/27], Loss: 0.016506\n",
      "Epoch [64/100] Complete:\n",
      "  Average Loss: 0.017332\n",
      "  Time: 15.77s\n",
      "  Learning Rate: 0.000287\n",
      "--------------------------------------------------\n",
      "Epoch [65/100], Batch [0/27], Loss: 0.016730\n",
      "Epoch [65/100], Batch [10/27], Loss: 0.016104\n",
      "Epoch [65/100], Batch [20/27], Loss: 0.016534\n",
      "Epoch [65/100] Complete:\n",
      "  Average Loss: 0.016846\n",
      "  Time: 14.07s\n",
      "  Learning Rate: 0.000273\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016846)\n",
      "Epoch [66/100], Batch [0/27], Loss: 0.015984\n",
      "Epoch [66/100], Batch [10/27], Loss: 0.016066\n",
      "Epoch [66/100], Batch [20/27], Loss: 0.016207\n",
      "Epoch [66/100] Complete:\n",
      "  Average Loss: 0.016914\n",
      "  Time: 17.24s\n",
      "  Learning Rate: 0.000259\n",
      "--------------------------------------------------\n",
      "Epoch [67/100], Batch [0/27], Loss: 0.016898\n",
      "Epoch [67/100], Batch [10/27], Loss: 0.017477\n",
      "Epoch [67/100], Batch [20/27], Loss: 0.018327\n",
      "Epoch [67/100] Complete:\n",
      "  Average Loss: 0.017414\n",
      "  Time: 18.30s\n",
      "  Learning Rate: 0.000245\n",
      "--------------------------------------------------\n",
      "Epoch [68/100], Batch [0/27], Loss: 0.016665\n",
      "Epoch [68/100], Batch [10/27], Loss: 0.015749\n",
      "Epoch [68/100], Batch [20/27], Loss: 0.015849\n",
      "Epoch [68/100] Complete:\n",
      "  Average Loss: 0.017120\n",
      "  Time: 17.30s\n",
      "  Learning Rate: 0.000232\n",
      "--------------------------------------------------\n",
      "Epoch [69/100], Batch [0/27], Loss: 0.015590\n",
      "Epoch [69/100], Batch [10/27], Loss: 0.015918\n",
      "Epoch [69/100], Batch [20/27], Loss: 0.015839\n",
      "Epoch [69/100] Complete:\n",
      "  Average Loss: 0.016911\n",
      "  Time: 19.13s\n",
      "  Learning Rate: 0.000219\n",
      "--------------------------------------------------\n",
      "Epoch [70/100], Batch [0/27], Loss: 0.016607\n",
      "Epoch [70/100], Batch [10/27], Loss: 0.015691\n",
      "Epoch [70/100], Batch [20/27], Loss: 0.016386\n",
      "Epoch [70/100] Complete:\n",
      "  Average Loss: 0.016985\n",
      "  Time: 16.23s\n",
      "  Learning Rate: 0.000206\n",
      "--------------------------------------------------\n",
      "Epoch [71/100], Batch [0/27], Loss: 0.016005\n",
      "Epoch [71/100], Batch [10/27], Loss: 0.016006\n",
      "Epoch [71/100], Batch [20/27], Loss: 0.015698\n",
      "Epoch [71/100] Complete:\n",
      "  Average Loss: 0.016913\n",
      "  Time: 16.54s\n",
      "  Learning Rate: 0.000194\n",
      "--------------------------------------------------\n",
      "Epoch [72/100], Batch [0/27], Loss: 0.017385\n",
      "Epoch [72/100], Batch [10/27], Loss: 0.023146\n",
      "Epoch [72/100], Batch [20/27], Loss: 0.016151\n",
      "Epoch [72/100] Complete:\n",
      "  Average Loss: 0.016620\n",
      "  Time: 18.33s\n",
      "  Learning Rate: 0.000181\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016620)\n",
      "Epoch [73/100], Batch [0/27], Loss: 0.016019\n",
      "Epoch [73/100], Batch [10/27], Loss: 0.016213\n",
      "Epoch [73/100], Batch [20/27], Loss: 0.019448\n",
      "Epoch [73/100] Complete:\n",
      "  Average Loss: 0.016714\n",
      "  Time: 15.34s\n",
      "  Learning Rate: 0.000169\n",
      "--------------------------------------------------\n",
      "Epoch [74/100], Batch [0/27], Loss: 0.015666\n",
      "Epoch [74/100], Batch [10/27], Loss: 0.016173\n",
      "Epoch [74/100], Batch [20/27], Loss: 0.015569\n",
      "Epoch [74/100] Complete:\n",
      "  Average Loss: 0.016460\n",
      "  Time: 15.41s\n",
      "  Learning Rate: 0.000158\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016460)\n",
      "Epoch [75/100], Batch [0/27], Loss: 0.015740\n",
      "Epoch [75/100], Batch [10/27], Loss: 0.015790\n",
      "Epoch [75/100], Batch [20/27], Loss: 0.016090\n",
      "Epoch [75/100] Complete:\n",
      "  Average Loss: 0.016628\n",
      "  Time: 17.07s\n",
      "  Learning Rate: 0.000146\n",
      "--------------------------------------------------\n",
      "Epoch [76/100], Batch [0/27], Loss: 0.019298\n",
      "Epoch [76/100], Batch [10/27], Loss: 0.016211\n",
      "Epoch [76/100], Batch [20/27], Loss: 0.015741\n",
      "Epoch [76/100] Complete:\n",
      "  Average Loss: 0.016577\n",
      "  Time: 15.60s\n",
      "  Learning Rate: 0.000136\n",
      "--------------------------------------------------\n",
      "Epoch [77/100], Batch [0/27], Loss: 0.015271\n",
      "Epoch [77/100], Batch [10/27], Loss: 0.016905\n",
      "Epoch [77/100], Batch [20/27], Loss: 0.015624\n",
      "Epoch [77/100] Complete:\n",
      "  Average Loss: 0.016754\n",
      "  Time: 17.30s\n",
      "  Learning Rate: 0.000125\n",
      "--------------------------------------------------\n",
      "Epoch [78/100], Batch [0/27], Loss: 0.015938\n",
      "Epoch [78/100], Batch [10/27], Loss: 0.016805\n",
      "Epoch [78/100], Batch [20/27], Loss: 0.015459\n",
      "Epoch [78/100] Complete:\n",
      "  Average Loss: 0.016537\n",
      "  Time: 15.80s\n",
      "  Learning Rate: 0.000115\n",
      "--------------------------------------------------\n",
      "Epoch [79/100], Batch [0/27], Loss: 0.015624\n",
      "Epoch [79/100], Batch [10/27], Loss: 0.016679\n",
      "Epoch [79/100], Batch [20/27], Loss: 0.015354\n",
      "Epoch [79/100] Complete:\n",
      "  Average Loss: 0.016351\n",
      "  Time: 18.03s\n",
      "  Learning Rate: 0.000105\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.016351)\n",
      "Epoch [80/100], Batch [0/27], Loss: 0.015369\n",
      "Epoch [80/100], Batch [10/27], Loss: 0.015702\n",
      "Epoch [80/100], Batch [20/27], Loss: 0.016779\n",
      "Epoch [80/100] Complete:\n",
      "  Average Loss: 0.015958\n",
      "  Time: 17.33s\n",
      "  Learning Rate: 0.000095\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.015958)\n",
      "  → Checkpoint saved at epoch 80\n",
      "Epoch [81/100], Batch [0/27], Loss: 0.015728\n",
      "Epoch [81/100], Batch [10/27], Loss: 0.016686\n",
      "Epoch [81/100], Batch [20/27], Loss: 0.017083\n",
      "Epoch [81/100] Complete:\n",
      "  Average Loss: 0.016302\n",
      "  Time: 16.64s\n",
      "  Learning Rate: 0.000086\n",
      "--------------------------------------------------\n",
      "Epoch [82/100], Batch [0/27], Loss: 0.015920\n",
      "Epoch [82/100], Batch [10/27], Loss: 0.016539\n",
      "Epoch [82/100], Batch [20/27], Loss: 0.015557\n",
      "Epoch [82/100] Complete:\n",
      "  Average Loss: 0.016021\n",
      "  Time: 15.62s\n",
      "  Learning Rate: 0.000078\n",
      "--------------------------------------------------\n",
      "Epoch [83/100], Batch [0/27], Loss: 0.016132\n",
      "Epoch [83/100], Batch [10/27], Loss: 0.018855\n",
      "Epoch [83/100], Batch [20/27], Loss: 0.015571\n",
      "Epoch [83/100] Complete:\n",
      "  Average Loss: 0.016568\n",
      "  Time: 17.49s\n",
      "  Learning Rate: 0.000070\n",
      "--------------------------------------------------\n",
      "Epoch [84/100], Batch [0/27], Loss: 0.015570\n",
      "Epoch [84/100], Batch [10/27], Loss: 0.015224\n",
      "Epoch [84/100], Batch [20/27], Loss: 0.015682\n",
      "Epoch [84/100] Complete:\n",
      "  Average Loss: 0.016784\n",
      "  Time: 17.22s\n",
      "  Learning Rate: 0.000062\n",
      "--------------------------------------------------\n",
      "Epoch [85/100], Batch [0/27], Loss: 0.015741\n",
      "Epoch [85/100], Batch [10/27], Loss: 0.015347\n",
      "Epoch [85/100], Batch [20/27], Loss: 0.015148\n",
      "Epoch [85/100] Complete:\n",
      "  Average Loss: 0.016342\n",
      "  Time: 15.35s\n",
      "  Learning Rate: 0.000054\n",
      "--------------------------------------------------\n",
      "Epoch [86/100], Batch [0/27], Loss: 0.015587\n",
      "Epoch [86/100], Batch [10/27], Loss: 0.021566\n",
      "Epoch [86/100], Batch [20/27], Loss: 0.017288\n",
      "Epoch [86/100] Complete:\n",
      "  Average Loss: 0.015980\n",
      "  Time: 15.88s\n",
      "  Learning Rate: 0.000048\n",
      "--------------------------------------------------\n",
      "Epoch [87/100], Batch [0/27], Loss: 0.015285\n",
      "Epoch [87/100], Batch [10/27], Loss: 0.016092\n",
      "Epoch [87/100], Batch [20/27], Loss: 0.016145\n",
      "Epoch [87/100] Complete:\n",
      "  Average Loss: 0.016061\n",
      "  Time: 15.14s\n",
      "  Learning Rate: 0.000041\n",
      "--------------------------------------------------\n",
      "Epoch [88/100], Batch [0/27], Loss: 0.016089\n",
      "Epoch [88/100], Batch [10/27], Loss: 0.018309\n",
      "Epoch [88/100], Batch [20/27], Loss: 0.015584\n",
      "Epoch [88/100] Complete:\n",
      "  Average Loss: 0.016272\n",
      "  Time: 17.46s\n",
      "  Learning Rate: 0.000035\n",
      "--------------------------------------------------\n",
      "Epoch [89/100], Batch [0/27], Loss: 0.015513\n",
      "Epoch [89/100], Batch [10/27], Loss: 0.015019\n",
      "Epoch [89/100], Batch [20/27], Loss: 0.015844\n",
      "Epoch [89/100] Complete:\n",
      "  Average Loss: 0.015917\n",
      "  Time: 15.76s\n",
      "  Learning Rate: 0.000030\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.015917)\n",
      "Epoch [90/100], Batch [0/27], Loss: 0.016731\n",
      "Epoch [90/100], Batch [10/27], Loss: 0.015527\n",
      "Epoch [90/100], Batch [20/27], Loss: 0.015645\n",
      "Epoch [90/100] Complete:\n",
      "  Average Loss: 0.017003\n",
      "  Time: 16.71s\n",
      "  Learning Rate: 0.000024\n",
      "--------------------------------------------------\n",
      "Epoch [91/100], Batch [0/27], Loss: 0.016860\n",
      "Epoch [91/100], Batch [10/27], Loss: 0.016882\n",
      "Epoch [91/100], Batch [20/27], Loss: 0.016895\n",
      "Epoch [91/100] Complete:\n",
      "  Average Loss: 0.015995\n",
      "  Time: 16.46s\n",
      "  Learning Rate: 0.000020\n",
      "--------------------------------------------------\n",
      "Epoch [92/100], Batch [0/27], Loss: 0.015114\n",
      "Epoch [92/100], Batch [10/27], Loss: 0.021180\n",
      "Epoch [92/100], Batch [20/27], Loss: 0.015201\n",
      "Epoch [92/100] Complete:\n",
      "  Average Loss: 0.016259\n",
      "  Time: 17.92s\n",
      "  Learning Rate: 0.000016\n",
      "--------------------------------------------------\n",
      "Epoch [93/100], Batch [0/27], Loss: 0.021988\n",
      "Epoch [93/100], Batch [10/27], Loss: 0.016121\n",
      "Epoch [93/100], Batch [20/27], Loss: 0.015321\n",
      "Epoch [93/100] Complete:\n",
      "  Average Loss: 0.016363\n",
      "  Time: 10.52s\n",
      "  Learning Rate: 0.000012\n",
      "--------------------------------------------------\n",
      "Epoch [94/100], Batch [0/27], Loss: 0.015399\n",
      "Epoch [94/100], Batch [10/27], Loss: 0.015767\n",
      "Epoch [94/100], Batch [20/27], Loss: 0.015606\n",
      "Epoch [94/100] Complete:\n",
      "  Average Loss: 0.016283\n",
      "  Time: 6.59s\n",
      "  Learning Rate: 0.000009\n",
      "--------------------------------------------------\n",
      "Epoch [95/100], Batch [0/27], Loss: 0.017889\n",
      "Epoch [95/100], Batch [10/27], Loss: 0.015119\n",
      "Epoch [95/100], Batch [20/27], Loss: 0.016138\n",
      "Epoch [95/100] Complete:\n",
      "  Average Loss: 0.016270\n",
      "  Time: 6.41s\n",
      "  Learning Rate: 0.000006\n",
      "--------------------------------------------------\n",
      "Epoch [96/100], Batch [0/27], Loss: 0.016316\n",
      "Epoch [96/100], Batch [10/27], Loss: 0.015664\n",
      "Epoch [96/100], Batch [20/27], Loss: 0.016056\n",
      "Epoch [96/100] Complete:\n",
      "  Average Loss: 0.016512\n",
      "  Time: 6.21s\n",
      "  Learning Rate: 0.000004\n",
      "--------------------------------------------------\n",
      "Epoch [97/100], Batch [0/27], Loss: 0.015222\n",
      "Epoch [97/100], Batch [10/27], Loss: 0.015439\n",
      "Epoch [97/100], Batch [20/27], Loss: 0.015999\n",
      "Epoch [97/100] Complete:\n",
      "  Average Loss: 0.016094\n",
      "  Time: 6.23s\n",
      "  Learning Rate: 0.000002\n",
      "--------------------------------------------------\n",
      "Epoch [98/100], Batch [0/27], Loss: 0.015821\n",
      "Epoch [98/100], Batch [10/27], Loss: 0.015650\n",
      "Epoch [98/100], Batch [20/27], Loss: 0.015913\n",
      "Epoch [98/100] Complete:\n",
      "  Average Loss: 0.015841\n",
      "  Time: 6.32s\n",
      "  Learning Rate: 0.000001\n",
      "--------------------------------------------------\n",
      "  → Best model saved! (Loss: 0.015841)\n",
      "Epoch [99/100], Batch [0/27], Loss: 0.015777\n",
      "Epoch [99/100], Batch [10/27], Loss: 0.016284\n",
      "Epoch [99/100], Batch [20/27], Loss: 0.016135\n",
      "Epoch [99/100] Complete:\n",
      "  Average Loss: 0.015882\n",
      "  Time: 6.29s\n",
      "  Learning Rate: 0.000000\n",
      "--------------------------------------------------\n",
      "Epoch [100/100], Batch [0/27], Loss: 0.016457\n",
      "Epoch [100/100], Batch [10/27], Loss: 0.016050\n",
      "Epoch [100/100], Batch [20/27], Loss: 0.027640\n",
      "Epoch [100/100] Complete:\n",
      "  Average Loss: 0.016847\n",
      "  Time: 6.32s\n",
      "  Learning Rate: 0.000000\n",
      "--------------------------------------------------\n",
      "  → Checkpoint saved at epoch 100\n",
      "\n",
      "============================================================\n",
      "Training completed for bottle!\n",
      "Best loss: 0.015841\n",
      "Model saved as: convnext_pure_bottle.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train the pure ConvNeXt RD model\n",
    "model, final_loss = train_convnext_rd(\n",
    "    class_name='bottle',\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure ConvNeXt RD Model Architecture:\n",
      "==================================================\n",
      "1. Teacher Encoder: ConvNeXt Tiny (Frozen)\n",
      "   - Outputs: [96, 192, 384] channels\n",
      "   - Features: LayerNorm + GELU + Depthwise Conv\n",
      "\n",
      "2. BN Layer: ConvNeXt Style Processing\n",
      "   - ConvNeXt blocks for feature fusion\n",
      "   - LayerNorm normalization\n",
      "   - Output: 768 channels\n",
      "\n",
      "3. Student Decoder: ConvNeXt Style Upsampling\n",
      "   - Progressive upsampling with ConvNeXt blocks\n",
      "   - LayerNorm + GELU consistency\n",
      "   - Outputs: [96, 192, 384] channels\n",
      "\n",
      "4. Loss: Normalized Cosine Similarity\n",
      "   - Feature normalization for stable training\n",
      "   - Multi-scale loss weighting\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"Pure ConvNeXt RD Model Architecture:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Teacher Encoder: ConvNeXt Tiny (Frozen)\")\n",
    "print(\"   - Outputs: [96, 192, 384] channels\")\n",
    "print(\"   - Features: LayerNorm + GELU + Depthwise Conv\")\n",
    "print(\"\\n2. BN Layer: ConvNeXt Style Processing\")\n",
    "print(\"   - ConvNeXt blocks for feature fusion\")\n",
    "print(\"   - LayerNorm normalization\")\n",
    "print(\"   - Output: 768 channels\")\n",
    "print(\"\\n3. Student Decoder: ConvNeXt Style Upsampling\")\n",
    "print(\"   - Progressive upsampling with ConvNeXt blocks\")\n",
    "print(\"   - LayerNorm + GELU consistency\")\n",
    "print(\"   - Outputs: [96, 192, 384] channels\")\n",
    "print(\"\\n4. Loss: Normalized Cosine Similarity\")\n",
    "print(\"   - Feature normalization for stable training\")\n",
    "print(\"   - Multi-scale loss weighting\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
