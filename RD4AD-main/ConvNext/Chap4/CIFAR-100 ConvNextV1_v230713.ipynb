{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a63d734",
   "metadata": {},
   "source": [
    "# 더욱 자세한 모델설명은 Pytorch 홈페이지 참조\n",
    "\n",
    "https://pytorch.org/vision/main/models/convnext.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b374a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 신경망들이 들어있는 모듈\n",
    "import torch.optim as optim # 최적화 알고리즘들이 들어있는 모듈\n",
    "import torch.nn.init as init # 텐서에 초기값을 줌\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms # transforms는 전처리를 위한 모듈\n",
    "\n",
    "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 모듈\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler #label과 이미지를 한번에 가져오기위한 Sampler 라이브러리\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # 시각화를 위한 모듈\n",
    "#=======================================================================================\n",
    "#from torchvision.models import convnext_tiny, convnext_small, convnext_base, convnext_large\n",
    "from ConvNext_V1 import convnext_tiny, convnext_small, convnext_base, convnext_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7cb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용할 평균과 표준 편차\n",
    "mean = [0.5071, 0.4867, 0.4408]\n",
    "std = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "#Image augmentation is used to train the model\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std),\n",
    "])\n",
    "#Only the data is normalaized we do not need to augment the test data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ab6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_imgs len :  50000\n",
      "test_imgs len :  10000\n"
     ]
    }
   ],
   "source": [
    "# Compose는 여러 transform을 적용할 수 있도록 묶어주는 역할\n",
    "train_imgs = ImageFolder(\"./Cifar_100/train_image\", transform = transform_train)\n",
    "test_imgs = ImageFolder(\"./Cifar_100/test_image\", transform = transform_test)\n",
    "\n",
    "print(\"train_imgs len : \", len(train_imgs)) #imagefolder객체\n",
    "print(\"test_imgs len : \", len(test_imgs)) #imagefolder객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2cf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 validation으로 나누기\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_imgs)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5464a592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d1d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0fd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders (combine dataset and sampler)\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 200\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, \n",
    "                                           batch_size = batch_size, \n",
    "                                           sampler = train_sampler, \n",
    "                                           num_workers = 3)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_imgs, \n",
    "                                           batch_size = batch_size, \n",
    "                                           sampler = valid_sampler, \n",
    "                                           num_workers = 3)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_imgs, \n",
    "                                          batch_size = batch_size, \n",
    "                                          num_workers = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25229a",
   "metadata": {},
   "source": [
    "# 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600f4e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cad73e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cai-tj\\AppData\\Local\\Temp/ipykernel_1796/828376984.py:8: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  plt.imshow(image.T) # imshow는 (m, n, 3)의 형태를 받으므로 Transpose\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image tensor 예시 :  tensor([[[-1.8957, -1.8957, -1.8957,  ..., -1.8957, -1.8957, -1.8957],\n",
      "         [ 0.1420,  0.1274,  0.0248,  ...,  0.6405,  0.0248, -1.8957],\n",
      "         [ 0.0248,  0.1127,  0.1274,  ...,  1.0363,  0.3619, -1.8957],\n",
      "         ...,\n",
      "         [-0.1805, -0.3417, -0.0485,  ...,  0.7871,  0.8018, -1.8957],\n",
      "         [ 0.0394, -0.2978, -0.1365,  ...,  0.8897,  0.9190, -1.8957],\n",
      "         [ 0.5379,  0.1127, -0.1805,  ...,  0.6845,  0.8897, -1.8957]],\n",
      "\n",
      "        [[-1.8975, -1.8975, -1.8975,  ..., -1.8975, -1.8975, -1.8975],\n",
      "         [-0.0169, -0.0322, -0.1393,  ...,  0.5640, -0.1087, -1.8975],\n",
      "         [-0.1393, -0.0475, -0.0322,  ...,  1.0838,  0.3194, -1.8975],\n",
      "         ...,\n",
      "         [-0.3839, -0.5673, -0.2616,  ...,  0.6405,  0.6558, -1.8975],\n",
      "         [-0.1393, -0.4909, -0.3227,  ...,  0.7475,  0.7781, -1.8975],\n",
      "         [ 0.3806, -0.0628, -0.3839,  ...,  0.5334,  0.7475, -1.8975]],\n",
      "\n",
      "        [[-1.5965, -1.5965, -1.5965,  ..., -1.5965, -1.5965, -1.5965],\n",
      "         [-0.0199, -0.0483, -0.1336,  ...,  0.4772, -0.1336, -1.5965],\n",
      "         [-0.1478, -0.0626, -0.0483,  ...,  0.9743,  0.2783, -1.5965],\n",
      "         ...,\n",
      "         [-0.5029, -0.6307, -0.3182,  ...,  0.4062,  0.3778, -1.5965],\n",
      "         [-0.3466, -0.6449, -0.4602,  ...,  0.4914,  0.5340, -1.5965],\n",
      "         [ 0.1079, -0.2614, -0.5313,  ...,  0.2925,  0.5198, -1.5965]]])\n",
      "\n",
      "이미지 크기 :  torch.Size([3, 32, 32])\n",
      "이미지 라벨 :  99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2650aca4910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcm0lEQVR4nO2dfXSd1XXmn41kS0ECGV/FsiV/yDI2Fcbxl6J4DBhiICEkkxBnxQ10EtowuO1K2qaT/kGhadLO6irT5ruTRWrCZyZATEIat0NiHJNgDGMC/rYRwh/IiiVbigW+IIFEZPb8octaxj3PkXwlXSk5z28tLV3tR+d9j47efd97z757b3N3CCF+9zlrrCcghCgMcnYhEkHOLkQiyNmFSAQ5uxCJIGcXIhGKhzPYzK4G8A0ARQC+4+63DfL7NM5XEhn3BrEXOmj4jiIL2ktKS+mY13t7qdZ3kv8FsWfh8Czi4ybk+bT+2pv5jSsi9rKJ/JKbUMxGAYiEiF99nV0h/No5m58Jv8lTG2li/7JpmQlB+8vd/ejpfTN4ieTt7GZWBOBbAK4CcATAM2a23t2fy+d4MyJaK7Hzf3H+xBZ43qSwU9ddcD4d89y+56nWnOWXTllkHrF/GnvaqX5HZFCEbT35jTuX2JdNn0THVFVO5gfs7afSL3YfoloLsdfzM6Ejoh2JaCMNv4UAf/7hKUH7N9d30jHDeRnfCOCAux9y9zcAPAjgI8M4nhBiFBmOs9cA+NUpPx/J2YQQ45DhvGcPvS/4T2+szGwNgDXDOI8QYgQYjrMfwdvfak8H0H76L7n7WgBrgfgGnRBidBnOy/hnAMw1s9lmNhHAJwCsH5lpCSFGmrzv7O7eb2afBbABA5GWu9x9X77H43utQHjfEYhtFMe02C5+bAe0pCS8Rz6lis0Q2PrUnsgROXVTzqHa/s5XqfYysXdEFqRqiHM6nVj4KkPsnW3H+aDIjntFOY9PzM3wUENr1+thO5/FuOG1iPYvd7cF7Xwvfphxdnd/BMAjwzmGEKIw6BN0QiSCnF2IRJCzC5EIcnYhEkHOLkQiWCELTsY+VDMxMq6a2TM8PFVWxgNDnR2/plq2j6d5ZUg4rH7hfDqmt7ePai8efJFqTe0nqBYLydSSxLGSk3xMJKqF2XNmUa3zGA/09JNgamYST3aZPYunQ9VNZ1cBUFzEcyb/+jsPBe08ePnbj7sHs950ZxciEeTsQiSCnF2IRJCzC5EIcnYhEmFYn40fSRaU8eedktLwbmtpCa9ZlqmsoFrVVL4j/MLBFqp1ZsN7uNlslo5paFxENZzk9ek6IrvxF0USP/q6w4kfXZHd+J5Ikkzv7sNUK4sUDqwju/hTMnzHvaaSl/eqykyj2qHDvPTX7/Ku+5miO7sQiSBnFyIR5OxCJIKcXYhEkLMLkQhydiESYdyE3iZnWA8RoGJSOIx2sp/XLOvr5fEknpoCnOznajEpUJft5ok1O7dvo1rrvgNU48E8oIrUVYsRq60Xq9fHA5hAppxrW58Lh+zKwEN5f3bde/k8ynlYruMwX+ORJnZ3zLNTVl5cQOwtkTG6swuRCHJ2IRJBzi5EIsjZhUgEObsQiSBnFyIRhhV6M7MWDCQWnQTQ7+4N+R4rm33ljMfUTOeZULHab/39PAWsehY/ZnV5OM2rjLSFAoBDzTy8ti078sGaipLw83dJpLbeJfMmUa2t7QTVerr5POqKwvOon9NIx1yz8lKq7d21i2o7f/kzql1F7LFwY76toV6KaLyCHm9vNm8eHzN3aTjz8VsbeCblSMTZ3+vukQZeQojxgF7GC5EIw3V2B/ComW0zszUjMSEhxOgw3JfxF7t7u5lNAbDRzJ53982n/kLuSUBPBEKMMcO6s7t7e+57J4AfAfhPuy/uvtbdG4azeSeEGD55O7uZlZnZOW89BvA+AHtHamJCiJFlOC/jqwD8yMzeOs797v7TvCdSwqsXsrBceaRv0YxIK6HYuJkX1FKtiIzbtOkJfq5JPJvv7HaevcZnD8ydV0O1zrajQXtV6QQ6hhX0BIC6ObzFVk83zzqsKK8N2lev/n06Zsb8uVTbuu77VNucR77Z9IgWqc0ZDaHFnImF1wCgnoTYtrzAx9zzwplnPubt7O5+CMDCfMcLIQqLQm9CJIKcXYhEkLMLkQhydiESQc4uRCKMm4KTPVkeSigrD4eNsid4ptyUKp71Nu8CXrywhFWVBPAP//ObQfuBSApVYzXvy7b60gVUy7Z18oNGmDmrNny8jnBIDgBaD3ZQraqK3w/a23jIq2xOOIDV0LiUjnmjmfdse3jjnVSLMZHYYxlqPDALtEe02RHtg5fy0OdtT/wmaB/pPnW6swuRCHJ2IRJBzi5EIsjZhUgEObsQiTBuduOzkZ5MXX3h3cp2hO0A0NS+g2pbNnOtNZIFUUW2aZdGdtyXXcxrrr17IU8teObxp6j2q+YWqhWXhpNTuk7waEdP5G/u6+A77h2R/9lNK5YH7WdHEoMevf0HVHuQnyoKS1ypj4yJ1aDbH9Hqi7j2MNlxB0Z+152hO7sQiSBnFyIR5OxCJIKcXYhEkLMLkQhydiESwdy9cCczoyfjqSk8+eC1yBheOW2geB7jkkYelKlfHg4ntZK6bwAwexavPrYgkpBzspsn+dx/911Uy54IN+fp512B0N7Ftd5IWC4DntzxxEMPB+1ndfMDXvNH11LtJ3waUS4g9nfPjNTWizSH6u7nochsZB33RsKUses4H9zdQnbd2YVIBDm7EIkgZxciEeTsQiSCnF2IRJCzC5EIg2a9mdldAD4EoNPdL8rZJgP4PoBaAC0AVrv7y8OZSP1Mrs0mYaMXI2XaIlEQXLaE9NsBsIxkawHAS2S5ior5MmaqeOht3gW8alnbQZ5f1bCcZ8s1bd8WtO/YdoKOKYlcBcWR+8H1qz9NtbOKwy2l/vXWz9Mx+YbXYrCw7QutPNds8aWTqFbSw0Oi2T4elpvBO2Whk0Qjh+VQAYZyZ78HwNWn2W4GsMnd5wLYlPtZCDGOGdTZc/3WTy/G+REA9+Ye3wvg2pGdlhBipMn3PXuVux8FgNz3WJNKIcQ4YNQr1ZjZGgBrRvs8Qog4+d7ZO8xsGgDkvtOtMndf6+4N7t6Q57mEECNAvs6+HsANucc3APjxyExHCDFaDCX09gCAywFUmtkRAF8EcBuAdWZ2Iwbq8318uBPpibRQYnX8qiN9eioix9uy/QUuloZDRgAwrzEclqvITKZjYgvc1ZWlWmfHr6lWVsr/8JlzwuG81oO8yGZ/N5Ww+vpPUe1Tq66l2mMb/iNo/0L7Hn6yCLGsyFiBSJbhWFHBx2x54kRe55qX4dp/ed9iqpUUh6/wdf/+LB0zl8x/b+R/Oaizu/t1RLpisLFCiPGDPkEnRCLI2YVIBDm7EIkgZxciEeTsQiTCuOn1tmhJDdVKSaipCJHYG3gIbcMjW6lWXMSXpLpmWtCe7eElA3siMcVDBw7wcSd4WC5GJhOeY/0FvOJkXWYu1f77quup1rKvmWrf/e4dQXvsguNd8YAFU/h96YFOnm3GVn9nZHmrIz3bSiMFOBEJ55VMraXaJcsvCtob3hO2A8Bf/M09QXukrqju7EKkgpxdiESQswuRCHJ2IRJBzi5EIsjZhUiEcRN6q5h0LtXq5oRDQ4cOsnKCQPY4D3mVl/PnuP3NPByGzU+F7aV8GYvBm3xVV/C/uSTSbyzbxTPiZtbMCNqrqsJ2ALjiPe+l2svNvI/d7f/4Nao90/N60F4fCU8dioTDdkbCa7FeaUxbwSOzWLCCazt2cW3nIa5Vz+f/zy2bw0VC6zJ8kixYGsnn1J1diFSQswuRCHJ2IRJBzi5EIsjZhUiEcbMb33mC76mWdpzeo2KAsgpe+61qFt/2raoNJ4sAQNdx3t6nuDw8x/5e3tunuJcv8dw5vNx+aTE/5t2PPcrH9YezOFav/AM6Zko5X48v3HoL1b7Zw7ef/7AufB8pmcR31Sv2UQk/40GNvMhGjtcR6R22YCm/P9YX11Lt3e/5PaoV9Yevuc7Dz/NzXRi2t0YiArqzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJIGcXIhGG0v7pLgAfAtDp7hflbF8CcBOAtzIybnH3R4YzkYc3Hqba7ExYu2QF7xVZVVtNtYpKnoDSsIKH7FZ+8kNhoSjccmmA2ogW4Q0edukAT4TJ9IaTJ86fzkM/P1kfbtUEAE928ljOykij7tlLw4k3Wx7m/+eXIvXd8o28saBiSSQhp7RsAtVmzuIJRcsW8gya1uYWqmWmhmspLl65kI7p7/1V0L75GO2xOqQ7+z0Arg7Yv+bui3Jfw3J0IcToM6izu/tmAOFPtQghfmsYznv2z5rZbjO7y8zOG7EZCSFGhXyd/XYAcwAsAnAUwFfYL5rZGjN71sx4/1khxKiTl7O7e4e7n3T3NwHcgUh9f3df6+4N7s5304QQo05ezm5mp25yfhTA3pGZjhBitBhK6O0BAJcDqDSzIwC+COByM1sEwAG0APjj4U4k1shpP8tCepq/M+g8zsNTvX28SU5FhsdkpswKB3LmLaVDMPHcWi7GmMjDeVeu/hgfdzAcevn1AR7K+7/rf0C1Ze+qo9plq5ZTra87PI/+5Xyvd+cTr1ItXKVtcJYtmRe0ZyM1/jqyPHx1WSUP6bYe5vULv/D1LVT70+vqg/ayCn5h1c8Ph1JLH49kbVIlh7tfFzDfOdg4IcT4Qp+gEyIR5OxCJIKcXYhEkLMLkQhydiESYdwUnKy/cBLV/v25E0H7olKenXRupJ1UbwcPvTXt4g10Hlm3MWjfsJlXSvz0X/MsqfMmnk81gLf+qZ0zn2qvHAsXqty6/ad0zMxI4cvuSHbYs9t4LyQ2+2ykxRP/r+RP76xwa6vHfvSveR1vXhUPAq76MG+jdSX92BkwpSa8yDsi6/tSVziE+dprfBV1ZxciEeTsQiSCnF2IRJCzC5EIcnYhEkHOLkQijJvQWyaSbXYeToTHVPJebxWT+PEyVTzU1BPp9db06Pag/eJVPEOt9zjv2QaeQBWnJDKw7edBc/vBSN+w+Tw82B4p9bh1H89sLuoPX1pNu3lmG89Di7Oi8Q+pdsVNXwza+6p4Ac7Hvv2XVOs4+DrVyor5Wt35wG1Uaz3YHrTfd8cP6ZgiEtw0GB2jO7sQiSBnFyIR5OxCJIKcXYhEkLMLkQjjZjf+ovk8KeS57eGWQZ1H+M45EG6PAwCLG3kiSW8v3239f+1hbfJjm+mYS1ZdS7Utjz1BtQ82LqHa2aWlVNv/ZDh5oqub136bOytSRC9yhfRHduP37G4L2sP7zgPUR+49C4tqqVa3/CqqvfsD4bqBH/vA5+iYP8/yBJS+J++hWmkpT75qPcyvx1pST25xI/+/HGoOr2Rx8TE6Rnd2IRJBzi5EIsjZhUgEObsQiSBnFyIR5OxCJMJQ2j/NAHAfgKkA3gSw1t2/YWaTAXwfQC0GWkCtdveX851ITWQm5SSXpKiPN41q2sZDHTv2hUN5AFAR60NFWL+9iWqPX/UBqn149TKqfXxVpGjZJh4a+vJ3/k/Q/iA/GhY230O1D67i/Th7cZJqFdXh+0hF/5t0zJTpPPxaBJ6sM3dpLdVYylA3HQEUZ8LhOgAomb+Ya5Hkmu4sDxO/0hG+wKun1tIx/eTanzjxEB0zlDt7P4DPu3s9gGUAPmNmFwK4GcAmd58LYFPuZyHEOGVQZ3f3o+6+Pff4VQBNAGoAfATAvblfuxfAtaM0RyHECHBG79nNrBbAYgBPA6hy96PAwBMCAJ4kLoQYc4b8cVkzKwfwQwCfc/dXzHiS/Gnj1gBYk9/0hBAjxZDu7GY2AQOO/j13fzhn7jCzaTl9GoBgU2t3X+vuDe7Od3qEEKPOoM5uA7fwOwE0uftXT5HWA7gh9/gGAD8e+ekJIUaKobyMvxjAJwHsMbOdOdstAG4DsM7MbgTQCuDjw5nINZHssPu+G66r9pOu43RMrLHS7CquferT11Dtxr95JGiPLWJRpLDat+/eSrXLa+6i2sxivj0SC7ExdvEIGvY89CzVlkV2aa7/1EeD9pJIxt5Lx4IvDgEAkyNhqPLyFqr1vHZR0N4UydibUc7Dtpd/eCHV+np4ZmF1DQ8dth4+GrQXF/Mrq7i4KGiPvbse1NndfQtAq9hdMdh4IcT4QJ+gEyIR5OxCJIKcXYhEkLMLkQhydiESYdwUnGw63EW1K1ZdGrS33/c4HdMeCSeV8+gPdjz5S6pFGjlRmiPayoi2bP4Kqt143Z+c8TyumjKBau9fzQs2ZiPZWvlQXn421eoW1lJtSiVPR3zxGC/4+bN//regvWYWbx125RweL62v4fOYOb2WasUl/HxdHeE1fukEn0dZWXgeZ53F79+6swuRCHJ2IRJBzi5EIsjZhUgEObsQiSBnFyIRxk3o7ev/ch/VysrC2USrrq+hYyrK+J9WEQn/3H83Lx7JjlhPRwC8NCRwxYV8ZNcxnnm1Djzbjz17Z6by0E+mkmuLl/Isr84OnqVWUVERtHd383DSnn0HqFZWygOfseywDJGyu16kY4qm8thszyR+romzZlMNx/qo9PimcOhwciUvfDmTnOusonA2HKA7uxDJIGcXIhHk7EIkgpxdiESQswuRCONmN/6f7vk21bZuuiNs3/wQHXPFysuoVjOJF0+78ct8N541LsqEN54BAJ8p4dott/0Z1R66/X6q3doY2cUvDmcA7Y/s7ndGar/Vz+ctjaojtfAmZ8I7/J1tv6ZjthzmO/VPPsXjGlMyPDnlpk9/LGjPhoshAwBO9nO36I9UNzzwNI8mFEfSqMrKw/N/6Thfq4qK8Pqe7OcZYLqzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJIGcXIhEGDb2Z2QwA9wGYioHo01p3/4aZfQnATQDeig/c4u7h/khDYMYCXgeNafNqV9Mxv9jwU6rtLOYJBvWRUFkxyY94/8oFdMyfvO9afsCpPIzz+FPbqLa1+3WqNZHIy6ILK+mYPW0tVJvSwsNa9edXU60Y4YmUIEvHZObwFklTImG+/m7edmnKkqVB++W1PPmnLNKzq/Xg81TramunWlMzr0a4Z1f4mGWlfO17e8NzZHZgaHH2fgCfd/ftZnYOgG1mtjGnfc3dvzyEYwghxpih9Ho7CuBo7vGrZtYEgOeWCiHGJWf0nt3MagEsBvB0zvRZM9ttZneZ2XkjPTkhxMgxZGc3s3IAPwTwOXd/BcDtAOYAWISBO/9XyLg1ZvasmfHev0KIUWdIzm5mEzDg6N9z94cBwN073P2ku78J4A4AjaGx7r7W3RvcvWGkJi2EOHMGdXYzMwB3Amhy96+eYj91S/ujAHh3eyHEmDOU3fiLAXwSwB4z25mz3QLgOjNbBMABtAD441GYX5SyOeGwCgD0lB/lWi8Pvf3R313PT3gsnNW0oJynvZ1bXku1R2/931T7VpaH12KcQ0KHmRoe1urt5eGwDY/+B9Xmnv8HVKubUxu093Txc1VV8stxceW5VAN4GO3FtnBYqzQSylt6Ga+719vMs+9a23hmYYyqSH1ARv/JcIht4IV2mKHsxm8BYAEp75i6EKLw6BN0QiSCnF2IRJCzC5EIcnYhEkHOLkQijJuCk3//IM+n+dtP/FXQXrbgQ3RMw6olVCsuvohqJeCZRnvvvy1o7zzcQsc8eWI71f7rxvVUm0gVIBPR6udXBe1zp/NwY+sBnq21v/k3VPvVAV60sSYTzujL9vL1ra6IFGXs4Zlt2UhLqZ628LiZK3jYFhP5WrX38fnvbOahtwpSCBQAysrCx+zp4X/Xiy3hc/W98QYdozu7EIkgZxciEeTsQiSCnF2IRJCzC5EIcnYhEmHchN5Ke1854zEZzKdaZ38v1Rou4KGVzn08RFJBsqtm1PAqlV//52BNDwAAD5IA/6Oxjmpbmg9RrYQUX5wXaUhXXcKzvGZO5f3L9jz3ItWeOxgOG53s4/NYdc1yql0yi6/xM9t+SbVM5TuD9ndW8hDaa628qCQiBR0vWc7DeR2R8GxVVXiOHR2811srCb3F0J1diESQswuRCHJ2IRJBzi5EIsjZhUgEObsQiTBuQm/VJ3jGE9AXtE4ED8f8t0sjWU0Rfm9pEdX2bpsdtD++/t/omB+cfJVqwdrbObL9PMurnddsRDHCWWqtu3jx35Ly/C6DHbtaqFZWET5m1VTeH67pCA+XltXwUFnppHDoCgCWNpKw4nEeutrx6E+odvlCnjE58dI/pVrLRt57sKMjnD14UeRcFZPCBTh/+sIWOkZ3diESQc4uRCLI2YVIBDm7EIkgZxciEQbdhjWzUgCbAZTkfv8H7v5FM5sM4PsAajHQ/mm1u7+c70Ru/Mt/pNqGzduC9iuu/306ZsG7eHJH18EjVJtXHmvFE44YbPj5z+mIcyJHm3lhJRcreKShByeo9hTZqS/dvoeOqa4K160DgPevWkm1TBXf0V73yNNBe/lUvsO8pZknfjxzmLfzWjCHt3LKPtIctM+dSoegp4ufK9sWjgwBwDtxJdU6OnjbqOLi8DVXWsojEA2N4aShs8t20jFDubP3AVjp7gsx0J75ajNbBuBmAJvcfS6ATbmfhRDjlEGd3Qfozv04IfflAD4C4N6c/V4A147GBIUQI8NQ+7MX5Tq4dgLY6O5PA6hy96MAkPvOX0sJIcacITm7u59090UApgNoNDP+xus0zGyNmT1rZs/mOUchxAhwRrvx7n4CwC8AXA2gw8ymAUDue/Azf+6+1t0b3L1heFMVQgyHQZ3dzN5pZpNyj98B4EoAzwNYD+CG3K/dAODHozRHIcQIYO4e/wWzd2FgA64IA08O69z9780sA2AdgJkAWgF83N159sbAseInE0IMG3e3kH1QZx9J5OxCjD7M2fUJOiESQc4uRCLI2YVIBDm7EIkgZxciEQpdg+44gMO5x5W5n8cazePtaB5v57dtHrOYUNDQ29tObPbsePhUneaheaQyD72MFyIR5OxCJMJYOvvaMTz3qWgeb0fzeDu/M/MYs/fsQojCopfxQiTCmDi7mV1tZs1mdsDMxqx2nZm1mNkeM9tZyOIaZnaXmXWa2d5TbJPNbKOZ7c99P2+M5vElM2vLrclOM7umAPOYYWY/N7MmM9tnZn+Rsxd0TSLzKOiamFmpmf3SzHbl5vF3Ofvw1sPdC/qFgVTZgwDqAEwEsAvAhYWeR24uLQAqx+C8KwAsAbD3FNs/Abg59/hmAP9rjObxJQB/VeD1mAZgSe7xOQBeAHBhodckMo+CrgkAA1CeezwBwNMAlg13Pcbizt4I4IC7H3L3NwA8iIHilcng7psBnJ77X/ACnmQeBcfdj7r79tzjVwE0AahBgdckMo+C4gOMeJHXsXD2GgCnFhw/gjFY0BwO4FEz22Zma8ZoDm8xngp4ftbMdude5o/624lTMbNaAIsxcDcbszU5bR5AgddkNIq8joWzhxLrxyokcLG7LwHwAQCfMbMVYzSP8cTtAOZgoEfAUQBfKdSJzawcwA8BfM7dXynUeYcwj4KviQ+jyCtjLJz9CIAZp/w8HUD7GMwD7t6e+94J4EeIt0wfbYZUwHO0cfeO3IX2JoA7UKA1MbMJGHCw77n7wzlzwdckNI+xWpPcuU/gDIu8MsbC2Z8BMNfMZpvZRACfwEDxyoJiZmVmds5bjwG8D8De+KhRZVwU8HzrYsrxURRgTczMANwJoMndv3qKVNA1YfMo9JqMWpHXQu0wnrbbeA0GdjoPArh1jOZQh4FIwC4A+wo5DwAPYODl4G8w8ErnRgAZDLTR2p/7PnmM5vFdAHsA7M5dXNMKMI9LMPBWbjeAnbmvawq9JpF5FHRNALwLwI7c+fYC+NucfVjroU/QCZEI+gSdEIkgZxciEeTsQiSCnF2IRJCzC5EIcnYhEkHOLkQiyNmFSIT/D5uzcF3HzXBVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_imgs[49998] # 랜덤한 이미지 한장 뽑아보기\n",
    "print (\"image tensor 예시 : \", image)\n",
    "\n",
    "# 컬러이므로 3채널, 32-32픽셀 이미지\n",
    "print(\"\\n이미지 크기 : \", image.shape)\n",
    "print(\"이미지 라벨 : \", label)\n",
    "\n",
    "plt.imshow(image.T) # imshow는 (m, n, 3)의 형태를 받으므로 Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b477b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0,\n",
       " 'aquarium_fish': 1,\n",
       " 'baby': 2,\n",
       " 'bear': 3,\n",
       " 'beaver': 4,\n",
       " 'bed': 5,\n",
       " 'bee': 6,\n",
       " 'beetle': 7,\n",
       " 'bicycle': 8,\n",
       " 'bottle': 9,\n",
       " 'bowl': 10,\n",
       " 'boy': 11,\n",
       " 'bridge': 12,\n",
       " 'bus': 13,\n",
       " 'butterfly': 14,\n",
       " 'camel': 15,\n",
       " 'can': 16,\n",
       " 'castle': 17,\n",
       " 'caterpillar': 18,\n",
       " 'cattle': 19,\n",
       " 'chair': 20,\n",
       " 'chimpanzee': 21,\n",
       " 'clock': 22,\n",
       " 'cloud': 23,\n",
       " 'cockroach': 24,\n",
       " 'couch': 25,\n",
       " 'crab': 26,\n",
       " 'crocodile': 27,\n",
       " 'cup': 28,\n",
       " 'dinosaur': 29,\n",
       " 'dolphin': 30,\n",
       " 'elephant': 31,\n",
       " 'flatfish': 32,\n",
       " 'forest': 33,\n",
       " 'fox': 34,\n",
       " 'girl': 35,\n",
       " 'hamster': 36,\n",
       " 'house': 37,\n",
       " 'kangaroo': 38,\n",
       " 'keyboard': 39,\n",
       " 'lamp': 40,\n",
       " 'lawn_mower': 41,\n",
       " 'leopard': 42,\n",
       " 'lion': 43,\n",
       " 'lizard': 44,\n",
       " 'lobster': 45,\n",
       " 'man': 46,\n",
       " 'maple_tree': 47,\n",
       " 'motorcycle': 48,\n",
       " 'mountain': 49,\n",
       " 'mouse': 50,\n",
       " 'mushroom': 51,\n",
       " 'oak_tree': 52,\n",
       " 'orange': 53,\n",
       " 'orchid': 54,\n",
       " 'otter': 55,\n",
       " 'palm_tree': 56,\n",
       " 'pear': 57,\n",
       " 'pickup_truck': 58,\n",
       " 'pine_tree': 59,\n",
       " 'plain': 60,\n",
       " 'plate': 61,\n",
       " 'poppy': 62,\n",
       " 'porcupine': 63,\n",
       " 'possum': 64,\n",
       " 'rabbit': 65,\n",
       " 'raccoon': 66,\n",
       " 'ray': 67,\n",
       " 'road': 68,\n",
       " 'rocket': 69,\n",
       " 'rose': 70,\n",
       " 'sea': 71,\n",
       " 'seal': 72,\n",
       " 'shark': 73,\n",
       " 'shrew': 74,\n",
       " 'skunk': 75,\n",
       " 'skyscraper': 76,\n",
       " 'snail': 77,\n",
       " 'snake': 78,\n",
       " 'spider': 79,\n",
       " 'squirrel': 80,\n",
       " 'streetcar': 81,\n",
       " 'sunflower': 82,\n",
       " 'sweet_pepper': 83,\n",
       " 'table': 84,\n",
       " 'tank': 85,\n",
       " 'telephone': 86,\n",
       " 'television': 87,\n",
       " 'tiger': 88,\n",
       " 'tractor': 89,\n",
       " 'train': 90,\n",
       " 'trout': 91,\n",
       " 'tulip': 92,\n",
       " 'turtle': 93,\n",
       " 'wardrobe': 94,\n",
       " 'whale': 95,\n",
       " 'willow_tree': 96,\n",
       " 'wolf': 97,\n",
       " 'woman': 98,\n",
       " 'worm': 99}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs.class_to_idx # 99번 라벨은 worm이므로 지렁이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38f9bd",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674f7ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75471066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa169bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7bd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48dda5",
   "metadata": {},
   "source": [
    "# 기본사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2667bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_base(weight = 'ConvNeXt_Base_Weights.IMAGENET1K_V1', num_classes = 100)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#Adam 대신 AdamW 사용\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c71765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.019372 \tValidation Loss: 3.800638\n",
      "Validation loss decreased (inf --> 3.800638).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 3.654326 \tValidation Loss: 3.484379\n",
      "Validation loss decreased (3.800638 --> 3.484379).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 3.386980 \tValidation Loss: 3.292136\n",
      "Validation loss decreased (3.484379 --> 3.292136).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.212203 \tValidation Loss: 3.123619\n",
      "Validation loss decreased (3.292136 --> 3.123619).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.066246 \tValidation Loss: 3.034568\n",
      "Validation loss decreased (3.123619 --> 3.034568).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 2.953108 \tValidation Loss: 2.938806\n",
      "Validation loss decreased (3.034568 --> 2.938806).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 2.836140 \tValidation Loss: 2.895538\n",
      "Validation loss decreased (2.938806 --> 2.895538).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 2.731141 \tValidation Loss: 2.772381\n",
      "Validation loss decreased (2.895538 --> 2.772381).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 2.619130 \tValidation Loss: 2.746059\n",
      "Validation loss decreased (2.772381 --> 2.746059).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 2.518988 \tValidation Loss: 2.680050\n",
      "Validation loss decreased (2.746059 --> 2.680050).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 2.417783 \tValidation Loss: 2.641482\n",
      "Validation loss decreased (2.680050 --> 2.641482).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 2.299923 \tValidation Loss: 2.613815\n",
      "Validation loss decreased (2.641482 --> 2.613815).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 2.184712 \tValidation Loss: 2.615483\n",
      "Epoch: 14 \tTraining Loss: 2.066569 \tValidation Loss: 2.595281\n",
      "Validation loss decreased (2.613815 --> 2.595281).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.924608 \tValidation Loss: 2.568456\n",
      "Validation loss decreased (2.595281 --> 2.568456).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.768133 \tValidation Loss: 2.599921\n",
      "Epoch: 17 \tTraining Loss: 1.598819 \tValidation Loss: 2.705213\n",
      "Epoch: 18 \tTraining Loss: 1.424762 \tValidation Loss: 2.722099\n",
      "Epoch: 19 \tTraining Loss: 1.237825 \tValidation Loss: 2.889103\n",
      "Epoch: 20 \tTraining Loss: 1.054297 \tValidation Loss: 2.949969\n",
      "Epoch: 21 \tTraining Loss: 0.881305 \tValidation Loss: 3.063709\n",
      "Epoch: 22 \tTraining Loss: 0.735155 \tValidation Loss: 3.174036\n",
      "Epoch: 23 \tTraining Loss: 0.616488 \tValidation Loss: 3.289423\n",
      "Epoch: 24 \tTraining Loss: 0.522223 \tValidation Loss: 3.500572\n",
      "Epoch: 25 \tTraining Loss: 0.443504 \tValidation Loss: 3.496528\n",
      "Epoch: 26 \tTraining Loss: 0.389597 \tValidation Loss: 3.611897\n",
      "Epoch: 27 \tTraining Loss: 0.354513 \tValidation Loss: 3.721514\n",
      "Epoch: 28 \tTraining Loss: 0.310558 \tValidation Loss: 3.843154\n",
      "Epoch: 29 \tTraining Loss: 0.289705 \tValidation Loss: 3.903849\n",
      "Epoch: 30 \tTraining Loss: 0.286304 \tValidation Loss: 3.976834\n",
      "Epoch: 31 \tTraining Loss: 0.248388 \tValidation Loss: 4.008685\n",
      "Epoch: 32 \tTraining Loss: 0.241661 \tValidation Loss: 4.099432\n",
      "Epoch: 33 \tTraining Loss: 0.225364 \tValidation Loss: 4.166672\n",
      "Epoch: 34 \tTraining Loss: 0.216887 \tValidation Loss: 4.229151\n",
      "Epoch: 35 \tTraining Loss: 0.213262 \tValidation Loss: 4.251236\n",
      "Epoch: 36 \tTraining Loss: 0.196494 \tValidation Loss: 4.277159\n",
      "Epoch: 37 \tTraining Loss: 0.185826 \tValidation Loss: 4.399383\n",
      "Epoch: 38 \tTraining Loss: 0.185989 \tValidation Loss: 4.460425\n",
      "Epoch: 39 \tTraining Loss: 0.176997 \tValidation Loss: 4.436493\n",
      "Epoch: 40 \tTraining Loss: 0.177737 \tValidation Loss: 4.506546\n",
      "Epoch: 41 \tTraining Loss: 0.166604 \tValidation Loss: 4.519863\n",
      "Epoch: 42 \tTraining Loss: 0.164489 \tValidation Loss: 4.549818\n",
      "Epoch: 43 \tTraining Loss: 0.157653 \tValidation Loss: 4.630653\n",
      "Epoch: 44 \tTraining Loss: 0.150650 \tValidation Loss: 4.581341\n",
      "Epoch: 45 \tTraining Loss: 0.150563 \tValidation Loss: 4.666879\n",
      "Epoch: 46 \tTraining Loss: 0.148984 \tValidation Loss: 4.736092\n",
      "Epoch: 47 \tTraining Loss: 0.146442 \tValidation Loss: 4.598392\n",
      "Epoch: 48 \tTraining Loss: 0.139405 \tValidation Loss: 4.740162\n",
      "Epoch: 49 \tTraining Loss: 0.144434 \tValidation Loss: 4.613693\n",
      "Epoch: 50 \tTraining Loss: 0.123124 \tValidation Loss: 4.789772\n",
      "Epoch: 51 \tTraining Loss: 0.129563 \tValidation Loss: 4.720475\n",
      "Epoch: 52 \tTraining Loss: 0.127128 \tValidation Loss: 4.830293\n",
      "Epoch: 53 \tTraining Loss: 0.128384 \tValidation Loss: 4.796531\n",
      "Epoch: 54 \tTraining Loss: 0.133580 \tValidation Loss: 4.778515\n",
      "Epoch: 55 \tTraining Loss: 0.109502 \tValidation Loss: 4.871713\n",
      "Epoch: 56 \tTraining Loss: 0.113938 \tValidation Loss: 4.840415\n",
      "Epoch: 57 \tTraining Loss: 0.112395 \tValidation Loss: 4.859639\n",
      "Epoch: 58 \tTraining Loss: 0.111489 \tValidation Loss: 4.957233\n",
      "Epoch: 59 \tTraining Loss: 0.119776 \tValidation Loss: 4.899949\n",
      "Epoch: 60 \tTraining Loss: 0.114638 \tValidation Loss: 4.898754\n",
      "Epoch: 61 \tTraining Loss: 0.120167 \tValidation Loss: 4.914101\n",
      "Epoch: 62 \tTraining Loss: 0.107906 \tValidation Loss: 4.924976\n",
      "Epoch: 63 \tTraining Loss: 0.104905 \tValidation Loss: 4.992917\n",
      "Epoch: 64 \tTraining Loss: 0.106396 \tValidation Loss: 4.957837\n",
      "Epoch: 65 \tTraining Loss: 0.100034 \tValidation Loss: 4.979095\n",
      "Epoch: 66 \tTraining Loss: 0.097919 \tValidation Loss: 4.957117\n",
      "Epoch: 67 \tTraining Loss: 0.095848 \tValidation Loss: 5.052164\n",
      "Epoch: 68 \tTraining Loss: 0.108666 \tValidation Loss: 5.089974\n",
      "Epoch: 69 \tTraining Loss: 0.104903 \tValidation Loss: 4.996053\n",
      "Epoch: 70 \tTraining Loss: 0.092181 \tValidation Loss: 5.092388\n",
      "Epoch: 71 \tTraining Loss: 0.082881 \tValidation Loss: 5.053424\n",
      "Epoch: 72 \tTraining Loss: 0.089649 \tValidation Loss: 5.078381\n",
      "Epoch: 73 \tTraining Loss: 0.083461 \tValidation Loss: 5.130583\n",
      "Epoch: 74 \tTraining Loss: 0.091250 \tValidation Loss: 5.153704\n",
      "Epoch: 75 \tTraining Loss: 0.095230 \tValidation Loss: 5.130250\n",
      "Epoch: 76 \tTraining Loss: 0.099625 \tValidation Loss: 5.120267\n",
      "Epoch: 77 \tTraining Loss: 0.081798 \tValidation Loss: 5.202754\n",
      "Epoch: 78 \tTraining Loss: 0.087515 \tValidation Loss: 5.101733\n",
      "Epoch: 79 \tTraining Loss: 0.084135 \tValidation Loss: 5.111336\n",
      "Epoch: 80 \tTraining Loss: 0.078264 \tValidation Loss: 5.185594\n",
      "Epoch: 81 \tTraining Loss: 0.081344 \tValidation Loss: 5.119610\n",
      "Epoch: 82 \tTraining Loss: 0.080421 \tValidation Loss: 5.131249\n",
      "Epoch: 83 \tTraining Loss: 0.079551 \tValidation Loss: 5.198808\n",
      "Epoch: 84 \tTraining Loss: 0.082508 \tValidation Loss: 5.185776\n",
      "Epoch: 85 \tTraining Loss: 0.085203 \tValidation Loss: 5.302741\n",
      "Epoch: 86 \tTraining Loss: 0.079457 \tValidation Loss: 5.212815\n",
      "Epoch: 87 \tTraining Loss: 0.082946 \tValidation Loss: 5.251962\n",
      "Epoch: 88 \tTraining Loss: 0.074398 \tValidation Loss: 5.347051\n",
      "Epoch: 89 \tTraining Loss: 0.077592 \tValidation Loss: 5.222801\n",
      "Epoch: 90 \tTraining Loss: 0.081113 \tValidation Loss: 5.272083\n",
      "Epoch: 91 \tTraining Loss: 0.070665 \tValidation Loss: 5.254495\n",
      "Epoch: 92 \tTraining Loss: 0.067519 \tValidation Loss: 5.234657\n",
      "Epoch: 93 \tTraining Loss: 0.068450 \tValidation Loss: 5.248584\n",
      "Epoch: 94 \tTraining Loss: 0.085970 \tValidation Loss: 5.225330\n",
      "Epoch: 95 \tTraining Loss: 0.079571 \tValidation Loss: 5.228209\n",
      "Epoch: 96 \tTraining Loss: 0.066042 \tValidation Loss: 5.297256\n",
      "Epoch: 97 \tTraining Loss: 0.063476 \tValidation Loss: 5.259495\n",
      "Epoch: 98 \tTraining Loss: 0.071504 \tValidation Loss: 5.329231\n",
      "Epoch: 99 \tTraining Loss: 0.075071 \tValidation Loss: 5.350619\n",
      "Epoch: 100 \tTraining Loss: 0.064744 \tValidation Loss: 5.330057\n",
      "Epoch: 101 \tTraining Loss: 0.067988 \tValidation Loss: 5.298532\n",
      "Epoch: 102 \tTraining Loss: 0.073666 \tValidation Loss: 5.329046\n",
      "Epoch: 103 \tTraining Loss: 0.064573 \tValidation Loss: 5.334126\n",
      "Epoch: 104 \tTraining Loss: 0.062148 \tValidation Loss: 5.333728\n",
      "Epoch: 105 \tTraining Loss: 0.070517 \tValidation Loss: 5.358047\n",
      "Epoch: 106 \tTraining Loss: 0.067168 \tValidation Loss: 5.332786\n",
      "Epoch: 107 \tTraining Loss: 0.058757 \tValidation Loss: 5.359950\n",
      "Epoch: 108 \tTraining Loss: 0.060510 \tValidation Loss: 5.410935\n",
      "Epoch: 109 \tTraining Loss: 0.063047 \tValidation Loss: 5.300473\n",
      "Epoch: 110 \tTraining Loss: 0.068887 \tValidation Loss: 5.386807\n",
      "Epoch: 111 \tTraining Loss: 0.068534 \tValidation Loss: 5.448249\n",
      "Epoch: 112 \tTraining Loss: 0.065766 \tValidation Loss: 5.356333\n",
      "Epoch: 113 \tTraining Loss: 0.062090 \tValidation Loss: 5.339626\n",
      "Epoch: 114 \tTraining Loss: 0.059943 \tValidation Loss: 5.417541\n",
      "Epoch: 115 \tTraining Loss: 0.060466 \tValidation Loss: 5.409264\n",
      "Epoch: 116 \tTraining Loss: 0.066089 \tValidation Loss: 5.349537\n",
      "Epoch: 117 \tTraining Loss: 0.059583 \tValidation Loss: 5.318692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 \tTraining Loss: 0.053933 \tValidation Loss: 5.441313\n",
      "Epoch: 119 \tTraining Loss: 0.054929 \tValidation Loss: 5.437253\n",
      "Epoch: 120 \tTraining Loss: 0.060600 \tValidation Loss: 5.405433\n",
      "Epoch: 121 \tTraining Loss: 0.061482 \tValidation Loss: 5.417670\n",
      "Epoch: 122 \tTraining Loss: 0.060691 \tValidation Loss: 5.418720\n",
      "Epoch: 123 \tTraining Loss: 0.061906 \tValidation Loss: 5.429848\n",
      "Epoch: 124 \tTraining Loss: 0.055459 \tValidation Loss: 5.411379\n",
      "Epoch: 125 \tTraining Loss: 0.057392 \tValidation Loss: 5.517505\n",
      "Epoch: 126 \tTraining Loss: 0.061894 \tValidation Loss: 5.395721\n",
      "Epoch: 127 \tTraining Loss: 0.051296 \tValidation Loss: 5.400082\n",
      "Epoch: 128 \tTraining Loss: 0.053204 \tValidation Loss: 5.341157\n",
      "Epoch: 129 \tTraining Loss: 0.055723 \tValidation Loss: 5.494224\n",
      "Epoch: 130 \tTraining Loss: 0.058578 \tValidation Loss: 5.371443\n",
      "Epoch: 131 \tTraining Loss: 0.054916 \tValidation Loss: 5.448200\n",
      "Epoch: 132 \tTraining Loss: 0.055003 \tValidation Loss: 5.501693\n",
      "Epoch: 133 \tTraining Loss: 0.048774 \tValidation Loss: 5.508843\n",
      "Epoch: 134 \tTraining Loss: 0.050745 \tValidation Loss: 5.504495\n",
      "Epoch: 135 \tTraining Loss: 0.049657 \tValidation Loss: 5.511144\n",
      "Epoch: 136 \tTraining Loss: 0.052381 \tValidation Loss: 5.503205\n",
      "Epoch: 137 \tTraining Loss: 0.052842 \tValidation Loss: 5.530614\n",
      "Epoch: 138 \tTraining Loss: 0.052946 \tValidation Loss: 5.529060\n",
      "Epoch: 139 \tTraining Loss: 0.050801 \tValidation Loss: 5.481491\n",
      "Epoch: 140 \tTraining Loss: 0.050014 \tValidation Loss: 5.447317\n",
      "Epoch: 141 \tTraining Loss: 0.048118 \tValidation Loss: 5.437328\n",
      "Epoch: 142 \tTraining Loss: 0.047446 \tValidation Loss: 5.406602\n",
      "Epoch: 143 \tTraining Loss: 0.053944 \tValidation Loss: 5.398269\n",
      "Epoch: 144 \tTraining Loss: 0.057356 \tValidation Loss: 5.570488\n",
      "Epoch: 145 \tTraining Loss: 0.045043 \tValidation Loss: 5.507138\n",
      "Epoch: 146 \tTraining Loss: 0.045539 \tValidation Loss: 5.569630\n",
      "Epoch: 147 \tTraining Loss: 0.049760 \tValidation Loss: 5.554929\n",
      "Epoch: 148 \tTraining Loss: 0.051232 \tValidation Loss: 5.554416\n",
      "Epoch: 149 \tTraining Loss: 0.047115 \tValidation Loss: 5.560792\n",
      "Epoch: 150 \tTraining Loss: 0.048358 \tValidation Loss: 5.573827\n",
      "Epoch: 151 \tTraining Loss: 0.053004 \tValidation Loss: 5.485212\n",
      "Epoch: 152 \tTraining Loss: 0.046283 \tValidation Loss: 5.589924\n",
      "Epoch: 153 \tTraining Loss: 0.049965 \tValidation Loss: 5.565024\n",
      "Epoch: 154 \tTraining Loss: 0.041643 \tValidation Loss: 5.544412\n",
      "Epoch: 155 \tTraining Loss: 0.047639 \tValidation Loss: 5.511740\n",
      "Epoch: 156 \tTraining Loss: 0.051433 \tValidation Loss: 5.551601\n",
      "Epoch: 157 \tTraining Loss: 0.046383 \tValidation Loss: 5.586450\n",
      "Epoch: 158 \tTraining Loss: 0.048748 \tValidation Loss: 5.614729\n",
      "Epoch: 159 \tTraining Loss: 0.040699 \tValidation Loss: 5.500156\n",
      "Epoch: 160 \tTraining Loss: 0.040058 \tValidation Loss: 5.539940\n",
      "Epoch: 161 \tTraining Loss: 0.052459 \tValidation Loss: 5.546212\n",
      "Epoch: 162 \tTraining Loss: 0.049880 \tValidation Loss: 5.527236\n",
      "Epoch: 163 \tTraining Loss: 0.046534 \tValidation Loss: 5.520224\n",
      "Epoch: 164 \tTraining Loss: 0.042118 \tValidation Loss: 5.548882\n",
      "Epoch: 165 \tTraining Loss: 0.045615 \tValidation Loss: 5.496548\n",
      "Epoch: 166 \tTraining Loss: 0.043533 \tValidation Loss: 5.598533\n",
      "Epoch: 167 \tTraining Loss: 0.042641 \tValidation Loss: 5.550649\n",
      "Epoch: 168 \tTraining Loss: 0.042029 \tValidation Loss: 5.609042\n",
      "Epoch: 169 \tTraining Loss: 0.046311 \tValidation Loss: 5.541530\n",
      "Epoch: 170 \tTraining Loss: 0.040427 \tValidation Loss: 5.541609\n",
      "Epoch: 171 \tTraining Loss: 0.045772 \tValidation Loss: 5.579878\n",
      "Epoch: 172 \tTraining Loss: 0.047168 \tValidation Loss: 5.523649\n",
      "Epoch: 173 \tTraining Loss: 0.039151 \tValidation Loss: 5.588343\n",
      "Epoch: 174 \tTraining Loss: 0.042817 \tValidation Loss: 5.550667\n",
      "Epoch: 175 \tTraining Loss: 0.041055 \tValidation Loss: 5.473560\n",
      "Epoch: 176 \tTraining Loss: 0.042372 \tValidation Loss: 5.646686\n",
      "Epoch: 177 \tTraining Loss: 0.046044 \tValidation Loss: 5.519741\n",
      "Epoch: 178 \tTraining Loss: 0.044290 \tValidation Loss: 5.527531\n",
      "Epoch: 179 \tTraining Loss: 0.037269 \tValidation Loss: 5.608255\n",
      "Epoch: 180 \tTraining Loss: 0.037500 \tValidation Loss: 5.605214\n",
      "Epoch: 181 \tTraining Loss: 0.039372 \tValidation Loss: 5.536793\n",
      "Epoch: 182 \tTraining Loss: 0.042578 \tValidation Loss: 5.576467\n",
      "Epoch: 183 \tTraining Loss: 0.037826 \tValidation Loss: 5.626835\n",
      "Epoch: 184 \tTraining Loss: 0.038331 \tValidation Loss: 5.655681\n",
      "Epoch: 185 \tTraining Loss: 0.038005 \tValidation Loss: 5.581133\n",
      "Epoch: 186 \tTraining Loss: 0.038797 \tValidation Loss: 5.656566\n",
      "Epoch: 187 \tTraining Loss: 0.040829 \tValidation Loss: 5.627185\n",
      "Epoch: 188 \tTraining Loss: 0.038431 \tValidation Loss: 5.633722\n",
      "Epoch: 189 \tTraining Loss: 0.042564 \tValidation Loss: 5.593251\n",
      "Epoch: 190 \tTraining Loss: 0.041854 \tValidation Loss: 5.659984\n",
      "Epoch: 191 \tTraining Loss: 0.035610 \tValidation Loss: 5.563545\n",
      "Epoch: 192 \tTraining Loss: 0.039114 \tValidation Loss: 5.528054\n",
      "Epoch: 193 \tTraining Loss: 0.036127 \tValidation Loss: 5.597708\n",
      "Epoch: 194 \tTraining Loss: 0.043192 \tValidation Loss: 5.668273\n",
      "Epoch: 195 \tTraining Loss: 0.040283 \tValidation Loss: 5.647249\n",
      "Epoch: 196 \tTraining Loss: 0.037740 \tValidation Loss: 5.559772\n",
      "Epoch: 197 \tTraining Loss: 0.040185 \tValidation Loss: 5.636432\n",
      "Epoch: 198 \tTraining Loss: 0.035415 \tValidation Loss: 5.653432\n",
      "Epoch: 199 \tTraining Loss: 0.039527 \tValidation Loss: 5.678136\n",
      "Epoch: 200 \tTraining Loss: 0.038987 \tValidation Loss: 5.609786\n"
     ]
    }
   ],
   "source": [
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = image.to(device)\n",
    "        target = label.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = loss_func(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################    \n",
    "    model.eval()   \n",
    "    for batch_idx, (image, label) in enumerate(valid_loader):\n",
    "        data = image.to(device)\n",
    "        target = label.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = loss_func(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'ConvNext_V1.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94478b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 13.969999313354492%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# evaluate model\n",
    "model.load_state_dict(torch.load('ConvNext_V1.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image,label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y= label.to(device)\n",
    "\n",
    "        output = model.forward(x)\n",
    "        #print(\"output\", output.shape)\n",
    "        #print(\"output\", output)\n",
    "        \n",
    "        # torch.max함수는 (최댓값,index)를 반환 \n",
    "        _,output_index = torch.max(output,1)\n",
    "        #print(\"_\",_.shape)\n",
    "        #print(\"_\",_)\n",
    "        #print(\"index\", output_index)\n",
    "        \n",
    "        # 전체 개수 += 라벨의 개수\n",
    "        total += label.size(0)\n",
    "        \n",
    "        # 도출한 모델의 index와 라벨이 일치하면 correct에 개수 추가\n",
    "        correct += (output_index == y).sum().float()\n",
    "    \n",
    "    # 정확도 도출\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e421fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes= train_imgs.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a97f454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "779f6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.523041\n",
      "\n",
      "Test Accuracy of apple: 75% (75/100)\n",
      "Test Accuracy of aquarium_fish: 48% (48/100)\n",
      "Test Accuracy of  baby: 37% (37/100)\n",
      "Test Accuracy of  bear: 13% (13/100)\n",
      "Test Accuracy of beaver: 35% (35/100)\n",
      "Test Accuracy of   bed: 37% (37/100)\n",
      "Test Accuracy of   bee: 49% (49/100)\n",
      "Test Accuracy of beetle: 41% (41/100)\n",
      "Test Accuracy of bicycle: 41% (41/100)\n",
      "Test Accuracy of bottle: 42% (42/100)\n",
      "Test Accuracy of  bowl: 17% (17/100)\n",
      "Test Accuracy of   boy: 22% (22/100)\n",
      "Test Accuracy of bridge: 45% (45/100)\n",
      "Test Accuracy of   bus: 32% (32/100)\n",
      "Test Accuracy of butterfly: 26% (26/100)\n",
      "Test Accuracy of camel: 25% (25/100)\n",
      "Test Accuracy of   can: 31% (31/100)\n",
      "Test Accuracy of castle: 66% (66/100)\n",
      "Test Accuracy of caterpillar: 44% (44/100)\n",
      "Test Accuracy of cattle: 27% (27/100)\n",
      "Test Accuracy of chair: 73% (73/100)\n",
      "Test Accuracy of chimpanzee: 67% (67/100)\n",
      "Test Accuracy of clock: 21% (21/100)\n",
      "Test Accuracy of cloud: 59% (59/100)\n",
      "Test Accuracy of cockroach: 66% (66/100)\n",
      "Test Accuracy of couch: 15% (15/100)\n",
      "Test Accuracy of  crab: 24% (24/100)\n",
      "Test Accuracy of crocodile: 19% (19/100)\n",
      "Test Accuracy of   cup: 43% (43/100)\n",
      "Test Accuracy of dinosaur: 25% (25/100)\n",
      "Test Accuracy of dolphin: 44% (44/100)\n",
      "Test Accuracy of elephant: 43% (43/100)\n",
      "Test Accuracy of flatfish: 33% (33/100)\n",
      "Test Accuracy of forest: 32% (32/100)\n",
      "Test Accuracy of   fox: 40% (40/100)\n",
      "Test Accuracy of  girl: 18% (18/100)\n",
      "Test Accuracy of hamster: 37% (37/100)\n",
      "Test Accuracy of house: 23% (23/100)\n",
      "Test Accuracy of kangaroo: 22% (22/100)\n",
      "Test Accuracy of keyboard: 36% (36/100)\n",
      "Test Accuracy of  lamp: 31% (31/100)\n",
      "Test Accuracy of lawn_mower: 64% (64/100)\n",
      "Test Accuracy of leopard: 25% (25/100)\n",
      "Test Accuracy of  lion: 44% (44/100)\n",
      "Test Accuracy of lizard: 20% (20/100)\n",
      "Test Accuracy of lobster: 20% (20/100)\n",
      "Test Accuracy of   man: 19% (19/100)\n",
      "Test Accuracy of maple_tree: 43% (43/100)\n",
      "Test Accuracy of motorcycle: 68% (68/100)\n",
      "Test Accuracy of mountain: 37% (37/100)\n",
      "Test Accuracy of mouse: 24% (24/100)\n",
      "Test Accuracy of mushroom: 44% (44/100)\n",
      "Test Accuracy of oak_tree: 86% (86/100)\n",
      "Test Accuracy of orange: 73% (73/100)\n",
      "Test Accuracy of orchid: 32% (32/100)\n",
      "Test Accuracy of otter: 12% (12/100)\n",
      "Test Accuracy of palm_tree: 43% (43/100)\n",
      "Test Accuracy of  pear: 35% (35/100)\n",
      "Test Accuracy of pickup_truck: 49% (49/100)\n",
      "Test Accuracy of pine_tree: 19% (19/100)\n",
      "Test Accuracy of plain: 67% (67/100)\n",
      "Test Accuracy of plate: 50% (50/100)\n",
      "Test Accuracy of poppy: 54% (54/100)\n",
      "Test Accuracy of porcupine: 36% (36/100)\n",
      "Test Accuracy of possum: 17% (17/100)\n",
      "Test Accuracy of rabbit: 26% (26/100)\n",
      "Test Accuracy of raccoon: 14% (14/100)\n",
      "Test Accuracy of   ray: 37% (37/100)\n",
      "Test Accuracy of  road: 82% (82/100)\n",
      "Test Accuracy of rocket: 59% (59/100)\n",
      "Test Accuracy of  rose: 31% (31/100)\n",
      "Test Accuracy of   sea: 44% (44/100)\n",
      "Test Accuracy of  seal: 11% (11/100)\n",
      "Test Accuracy of shark: 29% (29/100)\n",
      "Test Accuracy of shrew: 23% (23/100)\n",
      "Test Accuracy of skunk: 69% (69/100)\n",
      "Test Accuracy of skyscraper: 67% (67/100)\n",
      "Test Accuracy of snail: 26% (26/100)\n",
      "Test Accuracy of snake: 22% (22/100)\n",
      "Test Accuracy of spider: 33% (33/100)\n",
      "Test Accuracy of squirrel: 20% (20/100)\n",
      "Test Accuracy of streetcar: 20% (20/100)\n",
      "Test Accuracy of sunflower: 77% (77/100)\n",
      "Test Accuracy of sweet_pepper: 33% (33/100)\n",
      "Test Accuracy of table: 28% (28/100)\n",
      "Test Accuracy of  tank: 55% (55/100)\n",
      "Test Accuracy of telephone: 47% (47/100)\n",
      "Test Accuracy of television: 25% (25/100)\n",
      "Test Accuracy of tiger: 31% (31/100)\n",
      "Test Accuracy of tractor: 41% (41/100)\n",
      "Test Accuracy of train: 41% (41/100)\n",
      "Test Accuracy of trout: 51% (51/100)\n",
      "Test Accuracy of tulip: 26% (26/100)\n",
      "Test Accuracy of turtle: 26% (26/100)\n",
      "Test Accuracy of wardrobe: 70% (70/100)\n",
      "Test Accuracy of whale: 43% (43/100)\n",
      "Test Accuracy of willow_tree: 27% (27/100)\n",
      "Test Accuracy of  wolf: 41% (41/100)\n",
      "Test Accuracy of woman: 19% (19/100)\n",
      "Test Accuracy of  worm: 34% (34/100)\n",
      "\n",
      "Test Accuracy (Overall): 38.330000% (3833/10000)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model.load_state_dict(torch.load('ConvNext_V1.pt'))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for batch_idx, (image, label) in enumerate(test_loader):\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    data = image.to(device)\n",
    "    target = label.to(device)\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = loss_func(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "   \n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(100):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), \n",
    "                                                         np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),\n",
    "                                                      np.sum(class_correct), \n",
    "                                                      np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2559399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
